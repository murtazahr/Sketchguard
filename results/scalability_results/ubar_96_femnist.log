# Experiment: ubar k=96 dataset=femnist
# Timestamp: 2025-09-19T17:10:29.306708
# Command: python decentralized_fl_sim.py --dataset femnist --rounds 3 --local-epochs 1 --seed 987654321 --batch-size 64 --lr 0.01 --max-samples 10000 --agg ubar --ubar-rho 0.5 --attack-percentage 0.5 --attack-type directed_deviation --verbose --graph k-regular --k 96 --num-nodes 100
================================================================================

CUDAGraph supports dynamic shapes by recording a new graph for each distinct input size. Recording too many CUDAGraphs may lead to extra overhead. We have observed 51 distinct sizes. Please consider the following options for better performance: a) padding inputs to a few fixed number of shapes; or b) set torch._inductor.config.triton.cudagraph_skip_dynamic_graphs=True. Set torch._inductor.config.triton.cudagraph_dynamic_shape_warn_limit=None to silence this warning.
Device: cuda
Seed: 987654321
Loading 36 LEAF FEMNIST train files...
LEAF FEMNIST train: 3597 users, 734463 samples
Loading 36 LEAF FEMNIST test files...
LEAF FEMNIST test: 3597 users, 83388 samples
Found 3597 train users, 3597 test users, 3597 common users
User sample counts range: 525 (max) to 17 (min)
Distributed ALL 3597 users across 100 clients
Users per client: 35 (with 97 clients getting +1 user)
Train partition sizes: [8056, 7245, 7634, 7059, 7480, 6831, 7408, 8154, 7361, 7470, 6246, 6823, 7701, 6799, 7175, 7379, 7191, 7366, 6829, 7248, 6921, 6718, 7193, 8029, 7759, 7612, 7427, 7763, 6890, 8166, 7757, 7698, 8091, 7672, 7300, 6646, 7186, 7943, 8164, 7471, 7696, 7796, 7032, 8059, 7171, 8263, 7313, 6952, 7895, 7620, 7505, 7410, 6969, 7307, 7020, 7166, 7229, 7423, 7740, 7298, 6666, 7173, 6919, 7659, 6658, 7010, 7176, 8216, 7331, 7127, 7472, 7191, 6756, 6957, 6837, 7074, 7606, 7168, 6604, 6586, 7021, 8138, 7179, 7456, 6956, 8016, 7597, 7243, 7390, 7300, 7527, 7793, 6490, 8266, 7349, 7188, 7373, 6901, 6627, 7742]
Test partition sizes: [913, 824, 868, 802, 850, 776, 844, 924, 837, 847, 711, 776, 872, 774, 814, 835, 818, 836, 778, 822, 787, 762, 818, 909, 881, 866, 845, 879, 785, 929, 880, 873, 918, 869, 827, 755, 811, 900, 921, 849, 874, 883, 796, 909, 813, 936, 828, 790, 896, 863, 849, 843, 791, 829, 799, 816, 823, 843, 875, 830, 754, 813, 787, 869, 757, 799, 814, 934, 831, 809, 849, 818, 771, 792, 777, 806, 865, 816, 752, 747, 800, 921, 812, 844, 790, 910, 862, 822, 842, 829, 854, 883, 741, 933, 835, 817, 836, 788, 758, 880]
  Client 0: 8056 train samples, 62 unique classes
  Client 1: 7245 train samples, 62 unique classes
  Client 2: 7634 train samples, 62 unique classes
  Client 3: 7059 train samples, 62 unique classes
  Client 4: 7480 train samples, 62 unique classes
  Client 5: 6831 train samples, 62 unique classes
  Client 6: 7408 train samples, 62 unique classes
  Client 7: 8154 train samples, 62 unique classes
  Client 8: 7361 train samples, 62 unique classes
  Client 9: 7470 train samples, 62 unique classes
  Client 10: 6246 train samples, 62 unique classes
  Client 11: 6823 train samples, 62 unique classes
  Client 12: 7701 train samples, 62 unique classes
  Client 13: 6799 train samples, 62 unique classes
  Client 14: 7175 train samples, 62 unique classes
  Client 15: 7379 train samples, 62 unique classes
  Client 16: 7191 train samples, 62 unique classes
  Client 17: 7366 train samples, 62 unique classes
  Client 18: 6829 train samples, 62 unique classes
  Client 19: 7248 train samples, 62 unique classes
  Client 20: 6921 train samples, 62 unique classes
  Client 21: 6718 train samples, 62 unique classes
  Client 22: 7193 train samples, 62 unique classes
  Client 23: 8029 train samples, 62 unique classes
  Client 24: 7759 train samples, 62 unique classes
  Client 25: 7612 train samples, 62 unique classes
  Client 26: 7427 train samples, 62 unique classes
  Client 27: 7763 train samples, 62 unique classes
  Client 28: 6890 train samples, 62 unique classes
  Client 29: 8166 train samples, 62 unique classes
  Client 30: 7757 train samples, 62 unique classes
  Client 31: 7698 train samples, 62 unique classes
  Client 32: 8091 train samples, 62 unique classes
  Client 33: 7672 train samples, 62 unique classes
  Client 34: 7300 train samples, 62 unique classes
  Client 35: 6646 train samples, 62 unique classes
  Client 36: 7186 train samples, 62 unique classes
  Client 37: 7943 train samples, 62 unique classes
  Client 38: 8164 train samples, 62 unique classes
  Client 39: 7471 train samples, 62 unique classes
  Client 40: 7696 train samples, 62 unique classes
  Client 41: 7796 train samples, 62 unique classes
  Client 42: 7032 train samples, 62 unique classes
  Client 43: 8059 train samples, 62 unique classes
  Client 44: 7171 train samples, 62 unique classes
  Client 45: 8263 train samples, 62 unique classes
  Client 46: 7313 train samples, 62 unique classes
  Client 47: 6952 train samples, 62 unique classes
  Client 48: 7895 train samples, 62 unique classes
  Client 49: 7620 train samples, 62 unique classes
  Client 50: 7505 train samples, 62 unique classes
  Client 51: 7410 train samples, 62 unique classes
  Client 52: 6969 train samples, 62 unique classes
  Client 53: 7307 train samples, 62 unique classes
  Client 54: 7020 train samples, 62 unique classes
  Client 55: 7166 train samples, 62 unique classes
  Client 56: 7229 train samples, 62 unique classes
  Client 57: 7423 train samples, 62 unique classes
  Client 58: 7740 train samples, 62 unique classes
  Client 59: 7298 train samples, 62 unique classes
  Client 60: 6666 train samples, 62 unique classes
  Client 61: 7173 train samples, 62 unique classes
  Client 62: 6919 train samples, 62 unique classes
  Client 63: 7659 train samples, 62 unique classes
  Client 64: 6658 train samples, 62 unique classes
  Client 65: 7010 train samples, 62 unique classes
  Client 66: 7176 train samples, 62 unique classes
  Client 67: 8216 train samples, 62 unique classes
  Client 68: 7331 train samples, 62 unique classes
  Client 69: 7127 train samples, 62 unique classes
  Client 70: 7472 train samples, 62 unique classes
  Client 71: 7191 train samples, 62 unique classes
  Client 72: 6756 train samples, 62 unique classes
  Client 73: 6957 train samples, 62 unique classes
  Client 74: 6837 train samples, 62 unique classes
  Client 75: 7074 train samples, 62 unique classes
  Client 76: 7606 train samples, 62 unique classes
  Client 77: 7168 train samples, 62 unique classes
  Client 78: 6604 train samples, 62 unique classes
  Client 79: 6586 train samples, 62 unique classes
  Client 80: 7021 train samples, 62 unique classes
  Client 81: 8138 train samples, 62 unique classes
  Client 82: 7179 train samples, 62 unique classes
  Client 83: 7456 train samples, 62 unique classes
  Client 84: 6956 train samples, 62 unique classes
  Client 85: 8016 train samples, 62 unique classes
  Client 86: 7597 train samples, 62 unique classes
  Client 87: 7243 train samples, 62 unique classes
  Client 88: 7390 train samples, 62 unique classes
  Client 89: 7300 train samples, 62 unique classes
  Client 90: 7527 train samples, 62 unique classes
  Client 91: 7793 train samples, 62 unique classes
  Client 92: 6490 train samples, 62 unique classes
  Client 93: 8266 train samples, 62 unique classes
  Client 94: 7349 train samples, 62 unique classes
  Client 95: 7188 train samples, 62 unique classes
  Client 96: 7373 train samples, 62 unique classes
  Client 97: 6901 train samples, 62 unique classes
  Client 98: 6627 train samples, 62 unique classes
  Client 99: 7742 train samples, 62 unique classes
Will sample 10000 samples per client per epoch
Graph: k-regular, nodes: 100, edges: 4800
Degree statistics: avg=96.00, min=96, max=96
k-regular with k=96 (each node has exactly 96 neighbors)
Attack: Compromised 50/100 nodes: [0, 4, 6, 10, 13, 14, 15, 16, 18, 19, 22, 23, 24, 25, 27, 29, 30, 31, 32, 33, 35, 38, 40, 41, 45, 46, 47, 48, 49, 50, 52, 53, 55, 56, 57, 59, 61, 62, 66, 68, 73, 76, 77, 84, 85, 87, 88, 90, 92, 99]
Attack type: directed_deviation, lambda: 1.0
UBAR ALGORITHM (Two-Stage Byzantine-resilient)
  - Model dimension: 6,603,710 parameters
  - Rho parameter: 0.5
  - Stage 1: Distance-based filtering (select 50% closest neighbors)
  - Stage 2: Performance-based selection (loss comparison)
  - Complexity: O(deg(i)×d + deg(i)×inference)
Initial test acc across nodes: mean=0.0186 ± 0.0179
CUDAGraph supports dynamic shapes by recording a new graph for each distinct input size. Recording too many CUDAGraphs may lead to extra overhead. We have observed 51 distinct sizes. Please consider the following options for better performance: a) padding inputs to a few fixed number of shapes; or b) set torch._inductor.config.triton.cudagraph_skip_dynamic_graphs=True. Set torch._inductor.config.triton.cudagraph_dynamic_shape_warn_limit=None to silence this warning.
CUDAGraph supports dynamic shapes by recording a new graph for each distinct input size. Recording too many CUDAGraphs may lead to extra overhead. We have observed 51 distinct sizes. Please consider the following options for better performance: a) padding inputs to a few fixed number of shapes; or b) set torch._inductor.config.triton.cudagraph_skip_dynamic_graphs=True. Set torch._inductor.config.triton.cudagraph_dynamic_shape_warn_limit=None to silence this warning.
Round 001: test acc mean=0.0586 ± 0.0184 | min=0.0342 max=0.1314
         : test loss mean=4.0335 ± 0.0202
         : individual accs = ['0.050383', '0.041262', '0.100230', '0.104738', '0.057647', '0.063144', '0.049763', '0.067100', '0.070490', '0.044864', '0.059072', '0.064433', '0.037844', '0.047804', '0.072482', '0.034731', '0.072127', '0.065789', '0.065553', '0.053528', '0.052097', '0.051181', '0.037897', '0.035204', '0.048808', '0.058891', '0.054438', '0.055745', '0.047134', '0.037675', '0.043182', '0.054983', '0.044662', '0.054085', '0.050786', '0.104636', '0.088779', '0.045556', '0.051031', '0.064782', '0.045767', '0.041903', '0.070352', '0.052805', '0.045510', '0.034188', '0.053140', '0.054430', '0.049107', '0.038239', '0.061249', '0.052195', '0.067004', '0.060314', '0.131414', '0.080882', '0.052248', '0.051008', '0.099429', '0.046988', '0.053050', '0.050431', '0.048285', '0.057537', '0.070013', '0.045056', '0.047912', '0.058887', '0.073406', '0.044499', '0.057715', '0.050122', '0.058366', '0.044192', '0.043758', '0.088089', '0.045087', '0.050245', '0.062500', '0.105756', '0.057500', '0.051031', '0.059113', '0.050948', '0.051899', '0.041758', '0.053364', '0.077859', '0.071259', '0.051870', '0.039813', '0.073613', '0.116059', '0.039657', '0.069461', '0.047736', '0.041866', '0.064721', '0.062005', '0.094318']
         : correct/total = [(46, 913), (34, 824), (87, 868), (84, 802), (49, 850), (49, 776), (42, 844), (62, 924), (59, 837), (38, 847), (42, 711), (50, 776), (33, 872), (37, 774), (59, 814), (29, 835), (59, 818), (55, 836), (51, 778), (44, 822), (41, 787), (39, 762), (31, 818), (32, 909), (43, 881), (51, 866), (46, 845), (49, 879), (37, 785), (35, 929), (38, 880), (48, 873), (41, 918), (47, 869), (42, 827), (79, 755), (72, 811), (41, 900), (47, 921), (55, 849), (40, 874), (37, 883), (56, 796), (48, 909), (37, 813), (32, 936), (44, 828), (43, 790), (44, 896), (33, 863), (52, 849), (44, 843), (53, 791), (50, 829), (105, 799), (66, 816), (43, 823), (43, 843), (87, 875), (39, 830), (40, 754), (41, 813), (38, 787), (50, 869), (53, 757), (36, 799), (39, 814), (55, 934), (61, 831), (36, 809), (49, 849), (41, 818), (45, 771), (35, 792), (34, 777), (71, 806), (39, 865), (41, 816), (47, 752), (79, 747), (46, 800), (47, 921), (48, 812), (43, 844), (41, 790), (38, 910), (46, 862), (64, 822), (60, 842), (43, 829), (34, 854), (65, 883), (86, 741), (37, 933), (58, 835), (39, 817), (35, 836), (51, 788), (47, 758), (83, 880)]
         : compromised: 0.0556, honest: 0.0617
         : ubar stats = ['Node 0: s1=0.500, s2=0.688', 'Node 1: s1=0.500, s2=0.458', 'Node 2: s1=0.500, s2=0.562']...
Round 002: test acc mean=0.0532 ± 0.0109 | min=0.0349 max=0.0998
         : test loss mean=3.9770 ± 0.0261
         : individual accs = ['0.043812', '0.050971', '0.049539', '0.099751', '0.057647', '0.063144', '0.055687', '0.050866', '0.066906', '0.044864', '0.067511', '0.055412', '0.041284', '0.034884', '0.057740', '0.064671', '0.050122', '0.065789', '0.065553', '0.053528', '0.052097', '0.076115', '0.068460', '0.050605', '0.048808', '0.045035', '0.041420', '0.055745', '0.047134', '0.051668', '0.046591', '0.057274', '0.041394', '0.044879', '0.050786', '0.063576', '0.049322', '0.040000', '0.056460', '0.055359', '0.045767', '0.057758', '0.070352', '0.044004', '0.045510', '0.043803', '0.059179', '0.050633', '0.040179', '0.038239', '0.061249', '0.052195', '0.055626', '0.045838', '0.063830', '0.042892', '0.047388', '0.051008', '0.065143', '0.045783', '0.053050', '0.061501', '0.077510', '0.046030', '0.056803', '0.052566', '0.046683', '0.046039', '0.057762', '0.039555', '0.057715', '0.099022', '0.063554', '0.044192', '0.038610', '0.049628', '0.045087', '0.058824', '0.059840', '0.050870', '0.045000', '0.048860', '0.055419', '0.050948', '0.048101', '0.041758', '0.046404', '0.064477', '0.055819', '0.051870', '0.039813', '0.046433', '0.052632', '0.038585', '0.055090', '0.062424', '0.046651', '0.045685', '0.056728', '0.053409']
         : correct/total = [(40, 913), (42, 824), (43, 868), (80, 802), (49, 850), (49, 776), (47, 844), (47, 924), (56, 837), (38, 847), (48, 711), (43, 776), (36, 872), (27, 774), (47, 814), (54, 835), (41, 818), (55, 836), (51, 778), (44, 822), (41, 787), (58, 762), (56, 818), (46, 909), (43, 881), (39, 866), (35, 845), (49, 879), (37, 785), (48, 929), (41, 880), (50, 873), (38, 918), (39, 869), (42, 827), (48, 755), (40, 811), (36, 900), (52, 921), (47, 849), (40, 874), (51, 883), (56, 796), (40, 909), (37, 813), (41, 936), (49, 828), (40, 790), (36, 896), (33, 863), (52, 849), (44, 843), (44, 791), (38, 829), (51, 799), (35, 816), (39, 823), (43, 843), (57, 875), (38, 830), (40, 754), (50, 813), (61, 787), (40, 869), (43, 757), (42, 799), (38, 814), (43, 934), (48, 831), (32, 809), (49, 849), (81, 818), (49, 771), (35, 792), (30, 777), (40, 806), (39, 865), (48, 816), (45, 752), (38, 747), (36, 800), (45, 921), (45, 812), (43, 844), (38, 790), (38, 910), (40, 862), (53, 822), (47, 842), (43, 829), (34, 854), (41, 883), (39, 741), (36, 933), (46, 835), (51, 817), (39, 836), (36, 788), (43, 758), (47, 880)]
         : compromised: 0.0523, honest: 0.0541
         : ubar stats = ['Node 0: s1=0.500, s2=0.354', 'Node 1: s1=0.500, s2=0.667', 'Node 2: s1=0.500, s2=0.719']...
Round 003: test acc mean=0.0520 ± 0.0084 | min=0.0335 max=0.0761
         : test loss mean=3.8873 ± 0.0329
         : individual accs = ['0.044907', '0.041262', '0.064516', '0.058603', '0.057647', '0.063144', '0.049763', '0.033550', '0.050179', '0.044864', '0.045007', '0.056701', '0.047018', '0.047804', '0.057740', '0.034731', '0.050122', '0.065789', '0.062982', '0.055961', '0.054638', '0.076115', '0.068460', '0.050605', '0.048808', '0.045035', '0.041420', '0.039818', '0.059873', '0.051668', '0.048864', '0.053837', '0.041394', '0.074799', '0.050786', '0.063576', '0.043157', '0.064444', '0.056460', '0.055359', '0.048055', '0.057758', '0.042714', '0.044004', '0.045510', '0.043803', '0.059179', '0.055696', '0.059152', '0.038239', '0.061249', '0.052195', '0.055626', '0.063932', '0.053817', '0.047794', '0.055893', '0.053381', '0.046857', '0.057831', '0.053050', '0.059041', '0.055909', '0.046030', '0.056803', '0.058824', '0.047912', '0.046039', '0.054152', '0.053152', '0.056537', '0.046455', '0.070039', '0.039141', '0.043758', '0.044665', '0.045087', '0.042892', '0.062500', '0.050870', '0.045000', '0.044517', '0.055419', '0.050948', '0.048101', '0.051648', '0.059165', '0.059611', '0.047506', '0.060314', '0.039813', '0.052095', '0.060729', '0.039657', '0.047904', '0.040392', '0.041866', '0.045685', '0.056728', '0.053409']
         : correct/total = [(41, 913), (34, 824), (56, 868), (47, 802), (49, 850), (49, 776), (42, 844), (31, 924), (42, 837), (38, 847), (32, 711), (44, 776), (41, 872), (37, 774), (47, 814), (29, 835), (41, 818), (55, 836), (49, 778), (46, 822), (43, 787), (58, 762), (56, 818), (46, 909), (43, 881), (39, 866), (35, 845), (35, 879), (47, 785), (48, 929), (43, 880), (47, 873), (38, 918), (65, 869), (42, 827), (48, 755), (35, 811), (58, 900), (52, 921), (47, 849), (42, 874), (51, 883), (34, 796), (40, 909), (37, 813), (41, 936), (49, 828), (44, 790), (53, 896), (33, 863), (52, 849), (44, 843), (44, 791), (53, 829), (43, 799), (39, 816), (46, 823), (45, 843), (41, 875), (48, 830), (40, 754), (48, 813), (44, 787), (40, 869), (43, 757), (47, 799), (39, 814), (43, 934), (45, 831), (43, 809), (48, 849), (38, 818), (54, 771), (31, 792), (34, 777), (36, 806), (39, 865), (35, 816), (47, 752), (38, 747), (36, 800), (41, 921), (45, 812), (43, 844), (38, 790), (47, 910), (51, 862), (49, 822), (40, 842), (50, 829), (34, 854), (46, 883), (45, 741), (37, 933), (40, 835), (33, 817), (35, 836), (36, 788), (43, 758), (47, 880)]
         : compromised: 0.0523, honest: 0.0517
         : ubar stats = ['Node 0: s1=0.500, s2=0.243', 'Node 1: s1=0.500, s2=0.694', 'Node 2: s1=0.500, s2=0.618']...

=== FINAL RESULTS ===
Dataset: femnist, Nodes: 100, Graph: k-regular, Aggregation: ubar
Attack: directed_deviation, 50.0% compromised
Final accuracy - Compromised: 0.0523, Honest: 0.0517
Overall test accuracy: mean=0.0520 ± 0.0084

=== UBAR SUMMARY ===
Node 0: stage1=0.500, stage2=0.243, overall=0.122
Node 1: stage1=0.500, stage2=0.694, overall=0.347
Node 2: stage1=0.500, stage2=0.618, overall=0.309
Node 3: stage1=0.500, stage2=0.625, overall=0.312
Node 4: stage1=0.500, stage2=0.562, overall=0.281
Node 5: stage1=0.500, stage2=0.431, overall=0.215
Node 6: stage1=0.500, stage2=0.493, overall=0.247
Node 7: stage1=0.500, stage2=0.312, overall=0.156
Node 8: stage1=0.500, stage2=0.562, overall=0.281
Node 9: stage1=0.500, stage2=0.236, overall=0.118
Node 10: stage1=0.500, stage2=0.201, overall=0.101
Node 11: stage1=0.500, stage2=0.507, overall=0.253
Node 12: stage1=0.500, stage2=0.417, overall=0.208
Node 13: stage1=0.500, stage2=0.514, overall=0.257
Node 14: stage1=0.500, stage2=0.299, overall=0.149
Node 15: stage1=0.500, stage2=0.451, overall=0.226
Node 16: stage1=0.500, stage2=0.299, overall=0.149
Node 17: stage1=0.500, stage2=0.424, overall=0.212
Node 18: stage1=0.500, stage2=0.750, overall=0.375
Node 19: stage1=0.500, stage2=0.444, overall=0.222
Node 20: stage1=0.500, stage2=0.243, overall=0.122
Node 21: stage1=0.500, stage2=0.222, overall=0.111
Node 22: stage1=0.500, stage2=0.250, overall=0.125
Node 23: stage1=0.500, stage2=0.042, overall=0.021
Node 24: stage1=0.500, stage2=0.201, overall=0.101
Node 25: stage1=0.500, stage2=0.201, overall=0.101
Node 26: stage1=0.500, stage2=0.389, overall=0.194
Node 27: stage1=0.500, stage2=0.597, overall=0.299
Node 28: stage1=0.500, stage2=0.444, overall=0.222
Node 29: stage1=0.500, stage2=0.319, overall=0.160
Node 30: stage1=0.500, stage2=0.188, overall=0.094
Node 31: stage1=0.500, stage2=0.312, overall=0.156
Node 32: stage1=0.500, stage2=0.597, overall=0.299
Node 33: stage1=0.500, stage2=0.340, overall=0.170
Node 34: stage1=0.500, stage2=0.562, overall=0.281
Node 35: stage1=0.500, stage2=0.535, overall=0.267
Node 36: stage1=0.500, stage2=0.278, overall=0.139
Node 37: stage1=0.500, stage2=0.312, overall=0.156
Node 38: stage1=0.500, stage2=0.528, overall=0.264
Node 39: stage1=0.500, stage2=0.597, overall=0.299
Node 40: stage1=0.500, stage2=0.333, overall=0.167
Node 41: stage1=0.500, stage2=0.458, overall=0.229
Node 42: stage1=0.500, stage2=0.326, overall=0.163
Node 43: stage1=0.500, stage2=0.562, overall=0.281
Node 44: stage1=0.500, stage2=0.542, overall=0.271
Node 45: stage1=0.500, stage2=0.500, overall=0.250
Node 46: stage1=0.500, stage2=0.743, overall=0.372
Node 47: stage1=0.500, stage2=0.326, overall=0.163
Node 48: stage1=0.500, stage2=0.785, overall=0.392
Node 49: stage1=0.500, stage2=0.528, overall=0.264
Node 50: stage1=0.500, stage2=0.229, overall=0.115
Node 51: stage1=0.500, stage2=0.597, overall=0.299
Node 52: stage1=0.500, stage2=0.340, overall=0.170
Node 53: stage1=0.500, stage2=0.451, overall=0.226
Node 54: stage1=0.500, stage2=0.479, overall=0.240
Node 55: stage1=0.500, stage2=0.139, overall=0.069
Node 56: stage1=0.500, stage2=0.194, overall=0.097
Node 57: stage1=0.500, stage2=0.229, overall=0.115
Node 58: stage1=0.500, stage2=0.111, overall=0.056
Node 59: stage1=0.500, stage2=0.882, overall=0.441
Node 60: stage1=0.500, stage2=0.236, overall=0.118
Node 61: stage1=0.500, stage2=0.840, overall=0.420
Node 62: stage1=0.500, stage2=0.326, overall=0.163
Node 63: stage1=0.500, stage2=0.236, overall=0.118
Node 64: stage1=0.500, stage2=0.597, overall=0.299
Node 65: stage1=0.500, stage2=0.562, overall=0.281
Node 66: stage1=0.500, stage2=0.528, overall=0.264
Node 67: stage1=0.500, stage2=0.403, overall=0.201
Node 68: stage1=0.500, stage2=0.431, overall=0.215
Node 69: stage1=0.500, stage2=0.250, overall=0.125
Node 70: stage1=0.500, stage2=0.333, overall=0.167
Node 71: stage1=0.500, stage2=0.354, overall=0.177
Node 72: stage1=0.500, stage2=0.444, overall=0.222
Node 73: stage1=0.500, stage2=0.424, overall=0.212
Node 74: stage1=0.500, stage2=0.354, overall=0.177
Node 75: stage1=0.500, stage2=0.542, overall=0.271
Node 76: stage1=0.500, stage2=0.632, overall=0.316
Node 77: stage1=0.500, stage2=0.174, overall=0.087
Node 78: stage1=0.500, stage2=0.694, overall=0.347
Node 79: stage1=0.500, stage2=0.056, overall=0.028
Node 80: stage1=0.500, stage2=0.646, overall=0.323
Node 81: stage1=0.500, stage2=0.444, overall=0.222
Node 82: stage1=0.500, stage2=0.688, overall=0.344
Node 83: stage1=0.500, stage2=0.194, overall=0.097
Node 84: stage1=0.500, stage2=0.583, overall=0.292
Node 85: stage1=0.500, stage2=0.896, overall=0.448
Node 86: stage1=0.500, stage2=0.444, overall=0.222
Node 87: stage1=0.500, stage2=0.319, overall=0.160
Node 88: stage1=0.500, stage2=0.465, overall=0.233
Node 89: stage1=0.500, stage2=0.424, overall=0.212
Node 90: stage1=0.500, stage2=0.569, overall=0.285
Node 91: stage1=0.500, stage2=0.556, overall=0.278
Node 92: stage1=0.500, stage2=0.528, overall=0.264
Node 93: stage1=0.500, stage2=0.278, overall=0.139
Node 94: stage1=0.500, stage2=0.549, overall=0.274
Node 95: stage1=0.500, stage2=0.410, overall=0.205
Node 96: stage1=0.500, stage2=0.340, overall=0.170
Node 97: stage1=0.500, stage2=0.736, overall=0.368
Node 98: stage1=0.500, stage2=0.118, overall=0.059
Node 99: stage1=0.500, stage2=0.604, overall=0.302

=== PARALLEL EXECUTION TIME (realistic for distributed system) ===
  COMMUNICATION (max across nodes):
    - Full model transfer: 0.000s (0.0%)
  COMPUTATION (max across nodes):
    - Distance computation: 0.110s (2.2%)
    - Loss computation: 4.945s (97.4%)
    - Aggregation: 0.019s (0.4%)
  TOTALS:
    - Total computation: 5.075s (100.0%)
    - Total communication: 0.000s (0.0%)
    - Total parallel time: 5.075s

=== PER-NODE AVERAGE TIME ===
  - Distance computation: 0.108s
  - Loss computation: 4.587s
  - Aggregation: 0.011s
  - Model transfer: 0.000s
  - Total per node: 4.705s

=== TOTAL COMPUTATIONAL WORK (sum across all nodes) ===
  - Total distance computation: 10.762s
  - Total loss computation: 458.688s
  - Total aggregation: 1.062s
  - Total model transfer: 0.000s
  - Grand total: 470.512s
  - Mean Stage 1 acceptance rate: 0.500
  - Mean Stage 2 acceptance rate: 0.432
  - Overall acceptance rate: 0.216

UBAR Algorithm Properties:
  - Model dimension: 6,603,710
  - Rho parameter: 0.5
  - Two-stage approach: Distance filtering + loss evaluation
  - Stage 1 selects: 50% of neighbors
  - Stage 2 uses: Training sample loss comparison
  - Theoretical complexity: O(deg(i)×d + deg(i)×inference)
  - Approach: UBAR paper implementation


# Experiment completed successfully
