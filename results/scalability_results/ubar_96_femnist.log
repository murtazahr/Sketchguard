# Experiment: ubar k=96 dataset=femnist
# Timestamp: 2025-09-20T14:41:06.447349
# Device: CPU (forced for consistent performance)
# Command: python decentralized_fl_sim.py --dataset femnist --rounds 3 --local-epochs 1 --seed 987654321 --batch-size 64 --lr 0.01 --max-samples 10000 --agg ubar --ubar-rho 0.5 --attack-percentage 0.5 --attack-type directed_deviation --verbose --graph k-regular --k 96 --num-nodes 100
================================================================================

Device: cpu
Seed: 987654321
Loading 36 LEAF FEMNIST train files...
LEAF FEMNIST train: 3597 users, 734463 samples
Loading 36 LEAF FEMNIST test files...
LEAF FEMNIST test: 3597 users, 83388 samples
Found 3597 train users, 3597 test users, 3597 common users
User sample counts range: 525 (max) to 17 (min)
Distributed ALL 3597 users across 100 clients
Users per client: 35 (with 97 clients getting +1 user)
Train partition sizes: [8056, 7245, 7634, 7059, 7480, 6831, 7408, 8154, 7361, 7470, 6246, 6823, 7701, 6799, 7175, 7379, 7191, 7366, 6829, 7248, 6921, 6718, 7193, 8029, 7759, 7612, 7427, 7763, 6890, 8166, 7757, 7698, 8091, 7672, 7300, 6646, 7186, 7943, 8164, 7471, 7696, 7796, 7032, 8059, 7171, 8263, 7313, 6952, 7895, 7620, 7505, 7410, 6969, 7307, 7020, 7166, 7229, 7423, 7740, 7298, 6666, 7173, 6919, 7659, 6658, 7010, 7176, 8216, 7331, 7127, 7472, 7191, 6756, 6957, 6837, 7074, 7606, 7168, 6604, 6586, 7021, 8138, 7179, 7456, 6956, 8016, 7597, 7243, 7390, 7300, 7527, 7793, 6490, 8266, 7349, 7188, 7373, 6901, 6627, 7742]
Test partition sizes: [913, 824, 868, 802, 850, 776, 844, 924, 837, 847, 711, 776, 872, 774, 814, 835, 818, 836, 778, 822, 787, 762, 818, 909, 881, 866, 845, 879, 785, 929, 880, 873, 918, 869, 827, 755, 811, 900, 921, 849, 874, 883, 796, 909, 813, 936, 828, 790, 896, 863, 849, 843, 791, 829, 799, 816, 823, 843, 875, 830, 754, 813, 787, 869, 757, 799, 814, 934, 831, 809, 849, 818, 771, 792, 777, 806, 865, 816, 752, 747, 800, 921, 812, 844, 790, 910, 862, 822, 842, 829, 854, 883, 741, 933, 835, 817, 836, 788, 758, 880]
  Client 0: 8056 train samples, 62 unique classes
  Client 1: 7245 train samples, 62 unique classes
  Client 2: 7634 train samples, 62 unique classes
  Client 3: 7059 train samples, 62 unique classes
  Client 4: 7480 train samples, 62 unique classes
  Client 5: 6831 train samples, 62 unique classes
  Client 6: 7408 train samples, 62 unique classes
  Client 7: 8154 train samples, 62 unique classes
  Client 8: 7361 train samples, 62 unique classes
  Client 9: 7470 train samples, 62 unique classes
  Client 10: 6246 train samples, 62 unique classes
  Client 11: 6823 train samples, 62 unique classes
  Client 12: 7701 train samples, 62 unique classes
  Client 13: 6799 train samples, 62 unique classes
  Client 14: 7175 train samples, 62 unique classes
  Client 15: 7379 train samples, 62 unique classes
  Client 16: 7191 train samples, 62 unique classes
  Client 17: 7366 train samples, 62 unique classes
  Client 18: 6829 train samples, 62 unique classes
  Client 19: 7248 train samples, 62 unique classes
  Client 20: 6921 train samples, 62 unique classes
  Client 21: 6718 train samples, 62 unique classes
  Client 22: 7193 train samples, 62 unique classes
  Client 23: 8029 train samples, 62 unique classes
  Client 24: 7759 train samples, 62 unique classes
  Client 25: 7612 train samples, 62 unique classes
  Client 26: 7427 train samples, 62 unique classes
  Client 27: 7763 train samples, 62 unique classes
  Client 28: 6890 train samples, 62 unique classes
  Client 29: 8166 train samples, 62 unique classes
  Client 30: 7757 train samples, 62 unique classes
  Client 31: 7698 train samples, 62 unique classes
  Client 32: 8091 train samples, 62 unique classes
  Client 33: 7672 train samples, 62 unique classes
  Client 34: 7300 train samples, 62 unique classes
  Client 35: 6646 train samples, 62 unique classes
  Client 36: 7186 train samples, 62 unique classes
  Client 37: 7943 train samples, 62 unique classes
  Client 38: 8164 train samples, 62 unique classes
  Client 39: 7471 train samples, 62 unique classes
  Client 40: 7696 train samples, 62 unique classes
  Client 41: 7796 train samples, 62 unique classes
  Client 42: 7032 train samples, 62 unique classes
  Client 43: 8059 train samples, 62 unique classes
  Client 44: 7171 train samples, 62 unique classes
  Client 45: 8263 train samples, 62 unique classes
  Client 46: 7313 train samples, 62 unique classes
  Client 47: 6952 train samples, 62 unique classes
  Client 48: 7895 train samples, 62 unique classes
  Client 49: 7620 train samples, 62 unique classes
  Client 50: 7505 train samples, 62 unique classes
  Client 51: 7410 train samples, 62 unique classes
  Client 52: 6969 train samples, 62 unique classes
  Client 53: 7307 train samples, 62 unique classes
  Client 54: 7020 train samples, 62 unique classes
  Client 55: 7166 train samples, 62 unique classes
  Client 56: 7229 train samples, 62 unique classes
  Client 57: 7423 train samples, 62 unique classes
  Client 58: 7740 train samples, 62 unique classes
  Client 59: 7298 train samples, 62 unique classes
  Client 60: 6666 train samples, 62 unique classes
  Client 61: 7173 train samples, 62 unique classes
  Client 62: 6919 train samples, 62 unique classes
  Client 63: 7659 train samples, 62 unique classes
  Client 64: 6658 train samples, 62 unique classes
  Client 65: 7010 train samples, 62 unique classes
  Client 66: 7176 train samples, 62 unique classes
  Client 67: 8216 train samples, 62 unique classes
  Client 68: 7331 train samples, 62 unique classes
  Client 69: 7127 train samples, 62 unique classes
  Client 70: 7472 train samples, 62 unique classes
  Client 71: 7191 train samples, 62 unique classes
  Client 72: 6756 train samples, 62 unique classes
  Client 73: 6957 train samples, 62 unique classes
  Client 74: 6837 train samples, 62 unique classes
  Client 75: 7074 train samples, 62 unique classes
  Client 76: 7606 train samples, 62 unique classes
  Client 77: 7168 train samples, 62 unique classes
  Client 78: 6604 train samples, 62 unique classes
  Client 79: 6586 train samples, 62 unique classes
  Client 80: 7021 train samples, 62 unique classes
  Client 81: 8138 train samples, 62 unique classes
  Client 82: 7179 train samples, 62 unique classes
  Client 83: 7456 train samples, 62 unique classes
  Client 84: 6956 train samples, 62 unique classes
  Client 85: 8016 train samples, 62 unique classes
  Client 86: 7597 train samples, 62 unique classes
  Client 87: 7243 train samples, 62 unique classes
  Client 88: 7390 train samples, 62 unique classes
  Client 89: 7300 train samples, 62 unique classes
  Client 90: 7527 train samples, 62 unique classes
  Client 91: 7793 train samples, 62 unique classes
  Client 92: 6490 train samples, 62 unique classes
  Client 93: 8266 train samples, 62 unique classes
  Client 94: 7349 train samples, 62 unique classes
  Client 95: 7188 train samples, 62 unique classes
  Client 96: 7373 train samples, 62 unique classes
  Client 97: 6901 train samples, 62 unique classes
  Client 98: 6627 train samples, 62 unique classes
  Client 99: 7742 train samples, 62 unique classes
Will sample 10000 samples per client per epoch
Graph: k-regular, nodes: 100, edges: 4800
Degree statistics: avg=96.00, min=96, max=96
k-regular with k=96 (each node has exactly 96 neighbors)
Attack: Compromised 50/100 nodes: [0, 4, 6, 10, 13, 14, 15, 16, 18, 19, 22, 23, 24, 25, 27, 29, 30, 31, 32, 33, 35, 38, 40, 41, 45, 46, 47, 48, 49, 50, 52, 53, 55, 56, 57, 59, 61, 62, 66, 68, 73, 76, 77, 84, 85, 87, 88, 90, 92, 99]
Attack type: directed_deviation, lambda: 1.0
UBAR ALGORITHM (Two-Stage Byzantine-resilient)
  - Model dimension: 6,603,710 parameters
  - Rho parameter: 0.5
  - Stage 1: Distance-based filtering (select 50% closest neighbors)
  - Stage 2: Performance-based selection (loss comparison)
  - Complexity: O(deg(i)×d + deg(i)×inference)
Initial test acc across nodes: mean=0.0186 ± 0.0179
Round 001: test acc mean=0.0564 ± 0.0145 | min=0.0342 max=0.1022
         : test loss mean=4.0328 ± 0.0210
         : individual accs = ['0.050383', '0.041262', '0.099078', '0.102244', '0.057647', '0.063144', '0.049763', '0.063853', '0.078853', '0.044864', '0.088608', '0.064433', '0.037844', '0.047804', '0.057740', '0.034731', '0.051345', '0.046651', '0.065553', '0.053528', '0.052097', '0.051181', '0.037897', '0.035204', '0.048808', '0.058891', '0.072189', '0.055745', '0.047134', '0.051668', '0.069318', '0.083620', '0.044662', '0.077100', '0.050786', '0.100662', '0.071517', '0.041111', '0.071661', '0.064782', '0.045767', '0.041903', '0.070352', '0.053905', '0.045510', '0.034188', '0.053140', '0.054430', '0.049107', '0.076477', '0.061249', '0.052195', '0.039191', '0.060314', '0.053817', '0.084559', '0.051033', '0.056940', '0.060571', '0.046988', '0.053050', '0.050431', '0.049555', '0.057537', '0.070013', '0.045056', '0.047912', '0.041756', '0.055355', '0.044499', '0.057715', '0.050122', '0.058366', '0.041667', '0.043758', '0.044665', '0.045087', '0.050245', '0.062500', '0.087015', '0.057500', '0.051031', '0.064039', '0.050948', '0.051899', '0.046154', '0.040603', '0.059611', '0.067696', '0.051870', '0.039813', '0.046433', '0.066127', '0.039657', '0.070659', '0.047736', '0.041866', '0.064721', '0.056728', '0.088636']
         : correct/total = [(46, 913), (34, 824), (86, 868), (82, 802), (49, 850), (49, 776), (42, 844), (59, 924), (66, 837), (38, 847), (63, 711), (50, 776), (33, 872), (37, 774), (47, 814), (29, 835), (42, 818), (39, 836), (51, 778), (44, 822), (41, 787), (39, 762), (31, 818), (32, 909), (43, 881), (51, 866), (61, 845), (49, 879), (37, 785), (48, 929), (61, 880), (73, 873), (41, 918), (67, 869), (42, 827), (76, 755), (58, 811), (37, 900), (66, 921), (55, 849), (40, 874), (37, 883), (56, 796), (49, 909), (37, 813), (32, 936), (44, 828), (43, 790), (44, 896), (66, 863), (52, 849), (44, 843), (31, 791), (50, 829), (43, 799), (69, 816), (42, 823), (48, 843), (53, 875), (39, 830), (40, 754), (41, 813), (39, 787), (50, 869), (53, 757), (36, 799), (39, 814), (39, 934), (46, 831), (36, 809), (49, 849), (41, 818), (45, 771), (33, 792), (34, 777), (36, 806), (39, 865), (41, 816), (47, 752), (65, 747), (46, 800), (47, 921), (52, 812), (43, 844), (41, 790), (42, 910), (35, 862), (49, 822), (57, 842), (43, 829), (34, 854), (41, 883), (49, 741), (37, 933), (59, 835), (39, 817), (35, 836), (51, 788), (43, 758), (78, 880)]
         : compromised: 0.0562, honest: 0.0566
         : ubar stats = ['Node 0: s1=0.500, s2=0.375', 'Node 1: s1=0.500, s2=0.729', 'Node 2: s1=0.500, s2=0.208']...
Round 002: test acc mean=0.0509 ± 0.0080 | min=0.0349 max=0.0704
         : test loss mean=3.9723 ± 0.0308
         : individual accs = ['0.043812', '0.050971', '0.049539', '0.054863', '0.057647', '0.063144', '0.055687', '0.050866', '0.066906', '0.044864', '0.047820', '0.055412', '0.041284', '0.034884', '0.057740', '0.038323', '0.050122', '0.065789', '0.065553', '0.053528', '0.052097', '0.055118', '0.068460', '0.050605', '0.048808', '0.045035', '0.041420', '0.037543', '0.047134', '0.051668', '0.038636', '0.057274', '0.041394', '0.044879', '0.050786', '0.063576', '0.043157', '0.040000', '0.056460', '0.055359', '0.045767', '0.057758', '0.070352', '0.044004', '0.045510', '0.043803', '0.059179', '0.050633', '0.040179', '0.038239', '0.061249', '0.052195', '0.045512', '0.045838', '0.051314', '0.064951', '0.047388', '0.051008', '0.046857', '0.045783', '0.053050', '0.061501', '0.048285', '0.046030', '0.056803', '0.052566', '0.046683', '0.046039', '0.057762', '0.039555', '0.057715', '0.061125', '0.063554', '0.044192', '0.038610', '0.049628', '0.045087', '0.058824', '0.059840', '0.050870', '0.045000', '0.048860', '0.055419', '0.050948', '0.048101', '0.041758', '0.046404', '0.059611', '0.051069', '0.054282', '0.039813', '0.046433', '0.052632', '0.038585', '0.055090', '0.047736', '0.040670', '0.069797', '0.056728', '0.053409']
         : correct/total = [(40, 913), (42, 824), (43, 868), (44, 802), (49, 850), (49, 776), (47, 844), (47, 924), (56, 837), (38, 847), (34, 711), (43, 776), (36, 872), (27, 774), (47, 814), (32, 835), (41, 818), (55, 836), (51, 778), (44, 822), (41, 787), (42, 762), (56, 818), (46, 909), (43, 881), (39, 866), (35, 845), (33, 879), (37, 785), (48, 929), (34, 880), (50, 873), (38, 918), (39, 869), (42, 827), (48, 755), (35, 811), (36, 900), (52, 921), (47, 849), (40, 874), (51, 883), (56, 796), (40, 909), (37, 813), (41, 936), (49, 828), (40, 790), (36, 896), (33, 863), (52, 849), (44, 843), (36, 791), (38, 829), (41, 799), (53, 816), (39, 823), (43, 843), (41, 875), (38, 830), (40, 754), (50, 813), (38, 787), (40, 869), (43, 757), (42, 799), (38, 814), (43, 934), (48, 831), (32, 809), (49, 849), (50, 818), (49, 771), (35, 792), (30, 777), (40, 806), (39, 865), (48, 816), (45, 752), (38, 747), (36, 800), (45, 921), (45, 812), (43, 844), (38, 790), (38, 910), (40, 862), (49, 822), (43, 842), (45, 829), (34, 854), (41, 883), (39, 741), (36, 933), (46, 835), (39, 817), (34, 836), (55, 788), (43, 758), (47, 880)]
         : compromised: 0.0503, honest: 0.0514
         : ubar stats = ['Node 0: s1=0.500, s2=0.292', 'Node 1: s1=0.500, s2=0.656', 'Node 2: s1=0.500, s2=0.479']...
Round 003: test acc mean=0.0524 ± 0.0085 | min=0.0335 max=0.0761
         : test loss mean=3.8923 ± 0.0315
         : individual accs = ['0.058050', '0.041262', '0.064516', '0.058603', '0.057647', '0.063144', '0.049763', '0.033550', '0.050179', '0.044864', '0.052039', '0.056701', '0.064220', '0.047804', '0.057740', '0.034731', '0.050122', '0.065789', '0.062982', '0.055961', '0.054638', '0.076115', '0.068460', '0.050605', '0.048808', '0.045035', '0.041420', '0.039818', '0.059873', '0.051668', '0.048864', '0.053837', '0.041394', '0.074799', '0.055623', '0.063576', '0.043157', '0.064444', '0.056460', '0.055359', '0.048055', '0.041903', '0.042714', '0.042904', '0.045510', '0.043803', '0.047101', '0.055696', '0.059152', '0.038239', '0.061249', '0.052195', '0.055626', '0.063932', '0.053817', '0.063725', '0.055893', '0.053381', '0.049143', '0.057831', '0.053050', '0.059041', '0.049555', '0.046030', '0.056803', '0.058824', '0.047912', '0.046039', '0.054152', '0.039555', '0.056537', '0.061125', '0.063554', '0.044192', '0.037323', '0.044665', '0.045087', '0.061275', '0.062500', '0.050870', '0.045000', '0.044517', '0.055419', '0.050948', '0.048101', '0.051648', '0.059165', '0.059611', '0.047506', '0.057901', '0.039813', '0.052095', '0.060729', '0.041801', '0.047904', '0.055080', '0.041866', '0.045685', '0.047493', '0.053409']
         : correct/total = [(53, 913), (34, 824), (56, 868), (47, 802), (49, 850), (49, 776), (42, 844), (31, 924), (42, 837), (38, 847), (37, 711), (44, 776), (56, 872), (37, 774), (47, 814), (29, 835), (41, 818), (55, 836), (49, 778), (46, 822), (43, 787), (58, 762), (56, 818), (46, 909), (43, 881), (39, 866), (35, 845), (35, 879), (47, 785), (48, 929), (43, 880), (47, 873), (38, 918), (65, 869), (46, 827), (48, 755), (35, 811), (58, 900), (52, 921), (47, 849), (42, 874), (37, 883), (34, 796), (39, 909), (37, 813), (41, 936), (39, 828), (44, 790), (53, 896), (33, 863), (52, 849), (44, 843), (44, 791), (53, 829), (43, 799), (52, 816), (46, 823), (45, 843), (43, 875), (48, 830), (40, 754), (48, 813), (39, 787), (40, 869), (43, 757), (47, 799), (39, 814), (43, 934), (45, 831), (32, 809), (48, 849), (50, 818), (49, 771), (35, 792), (29, 777), (36, 806), (39, 865), (50, 816), (47, 752), (38, 747), (36, 800), (41, 921), (45, 812), (43, 844), (38, 790), (47, 910), (51, 862), (49, 822), (40, 842), (48, 829), (34, 854), (46, 883), (45, 741), (39, 933), (40, 835), (45, 817), (35, 836), (36, 788), (36, 758), (47, 880)]
         : compromised: 0.0528, honest: 0.0520
         : ubar stats = ['Node 0: s1=0.500, s2=0.319', 'Node 1: s1=0.500, s2=0.542', 'Node 2: s1=0.500, s2=0.583']...

=== FINAL RESULTS ===
Dataset: femnist, Nodes: 100, Graph: k-regular, Aggregation: ubar
Attack: directed_deviation, 50.0% compromised
Final accuracy - Compromised: 0.0528, Honest: 0.0520
Overall test accuracy: mean=0.0524 ± 0.0085

=== UBAR SUMMARY ===
Node 0: stage1=0.500, stage2=0.319, overall=0.160
Node 1: stage1=0.500, stage2=0.542, overall=0.271
Node 2: stage1=0.500, stage2=0.583, overall=0.292
Node 3: stage1=0.500, stage2=0.583, overall=0.292
Node 4: stage1=0.500, stage2=0.583, overall=0.292
Node 5: stage1=0.500, stage2=0.556, overall=0.278
Node 6: stage1=0.500, stage2=0.590, overall=0.295
Node 7: stage1=0.500, stage2=0.424, overall=0.212
Node 8: stage1=0.500, stage2=0.444, overall=0.222
Node 9: stage1=0.500, stage2=0.312, overall=0.156
Node 10: stage1=0.500, stage2=0.069, overall=0.035
Node 11: stage1=0.500, stage2=0.667, overall=0.333
Node 12: stage1=0.500, stage2=0.292, overall=0.146
Node 13: stage1=0.500, stage2=0.750, overall=0.375
Node 14: stage1=0.500, stage2=0.597, overall=0.299
Node 15: stage1=0.500, stage2=0.389, overall=0.194
Node 16: stage1=0.500, stage2=0.396, overall=0.198
Node 17: stage1=0.500, stage2=0.264, overall=0.132
Node 18: stage1=0.500, stage2=0.340, overall=0.170
Node 19: stage1=0.500, stage2=0.410, overall=0.205
Node 20: stage1=0.500, stage2=0.326, overall=0.163
Node 21: stage1=0.500, stage2=0.312, overall=0.156
Node 22: stage1=0.500, stage2=0.306, overall=0.153
Node 23: stage1=0.500, stage2=0.500, overall=0.250
Node 24: stage1=0.500, stage2=0.319, overall=0.160
Node 25: stage1=0.500, stage2=0.361, overall=0.181
Node 26: stage1=0.500, stage2=0.486, overall=0.243
Node 27: stage1=0.500, stage2=0.403, overall=0.201
Node 28: stage1=0.500, stage2=0.576, overall=0.288
Node 29: stage1=0.500, stage2=0.590, overall=0.295
Node 30: stage1=0.500, stage2=0.312, overall=0.156
Node 31: stage1=0.500, stage2=0.354, overall=0.177
Node 32: stage1=0.500, stage2=0.549, overall=0.274
Node 33: stage1=0.500, stage2=0.361, overall=0.181
Node 34: stage1=0.500, stage2=0.778, overall=0.389
Node 35: stage1=0.500, stage2=0.604, overall=0.302
Node 36: stage1=0.500, stage2=0.243, overall=0.122
Node 37: stage1=0.500, stage2=0.264, overall=0.132
Node 38: stage1=0.500, stage2=0.486, overall=0.243
Node 39: stage1=0.500, stage2=0.403, overall=0.201
Node 40: stage1=0.500, stage2=0.243, overall=0.122
Node 41: stage1=0.500, stage2=0.354, overall=0.177
Node 42: stage1=0.500, stage2=0.229, overall=0.115
Node 43: stage1=0.500, stage2=0.438, overall=0.219
Node 44: stage1=0.500, stage2=0.562, overall=0.281
Node 45: stage1=0.500, stage2=0.826, overall=0.413
Node 46: stage1=0.500, stage2=0.368, overall=0.184
Node 47: stage1=0.500, stage2=0.222, overall=0.111
Node 48: stage1=0.500, stage2=0.632, overall=0.316
Node 49: stage1=0.500, stage2=0.347, overall=0.174
Node 50: stage1=0.500, stage2=0.438, overall=0.219
Node 51: stage1=0.500, stage2=0.472, overall=0.236
Node 52: stage1=0.500, stage2=0.174, overall=0.087
Node 53: stage1=0.500, stage2=0.333, overall=0.167
Node 54: stage1=0.500, stage2=0.118, overall=0.059
Node 55: stage1=0.500, stage2=0.319, overall=0.160
Node 56: stage1=0.500, stage2=0.229, overall=0.115
Node 57: stage1=0.500, stage2=0.139, overall=0.069
Node 58: stage1=0.500, stage2=0.222, overall=0.111
Node 59: stage1=0.500, stage2=0.743, overall=0.372
Node 60: stage1=0.500, stage2=0.597, overall=0.299
Node 61: stage1=0.500, stage2=0.542, overall=0.271
Node 62: stage1=0.500, stage2=0.306, overall=0.153
Node 63: stage1=0.500, stage2=0.507, overall=0.253
Node 64: stage1=0.500, stage2=0.493, overall=0.247
Node 65: stage1=0.500, stage2=0.562, overall=0.281
Node 66: stage1=0.500, stage2=0.438, overall=0.219
Node 67: stage1=0.500, stage2=0.472, overall=0.236
Node 68: stage1=0.500, stage2=0.271, overall=0.135
Node 69: stage1=0.500, stage2=0.319, overall=0.160
Node 70: stage1=0.500, stage2=0.361, overall=0.181
Node 71: stage1=0.500, stage2=0.521, overall=0.260
Node 72: stage1=0.500, stage2=0.625, overall=0.312
Node 73: stage1=0.500, stage2=0.028, overall=0.014
Node 74: stage1=0.500, stage2=0.417, overall=0.208
Node 75: stage1=0.500, stage2=0.653, overall=0.326
Node 76: stage1=0.500, stage2=0.549, overall=0.274
Node 77: stage1=0.500, stage2=0.340, overall=0.170
Node 78: stage1=0.500, stage2=0.632, overall=0.316
Node 79: stage1=0.500, stage2=0.208, overall=0.104
Node 80: stage1=0.500, stage2=0.826, overall=0.413
Node 81: stage1=0.500, stage2=0.549, overall=0.274
Node 82: stage1=0.500, stage2=0.583, overall=0.292
Node 83: stage1=0.500, stage2=0.528, overall=0.264
Node 84: stage1=0.500, stage2=0.632, overall=0.316
Node 85: stage1=0.500, stage2=0.271, overall=0.135
Node 86: stage1=0.500, stage2=0.368, overall=0.184
Node 87: stage1=0.500, stage2=0.215, overall=0.108
Node 88: stage1=0.500, stage2=0.167, overall=0.083
Node 89: stage1=0.500, stage2=0.396, overall=0.198
Node 90: stage1=0.500, stage2=0.694, overall=0.347
Node 91: stage1=0.500, stage2=0.521, overall=0.260
Node 92: stage1=0.500, stage2=0.299, overall=0.149
Node 93: stage1=0.500, stage2=0.236, overall=0.118
Node 94: stage1=0.500, stage2=0.486, overall=0.243
Node 95: stage1=0.500, stage2=0.375, overall=0.188
Node 96: stage1=0.500, stage2=0.208, overall=0.104
Node 97: stage1=0.500, stage2=0.604, overall=0.302
Node 98: stage1=0.500, stage2=0.083, overall=0.042
Node 99: stage1=0.500, stage2=0.771, overall=0.385

=== PARALLEL EXECUTION TIME (realistic for distributed system) ===
  COMMUNICATION (max across nodes):
    - Full model transfer: 0.000s (0.0%)
  COMPUTATION (max across nodes):
    - Distance computation: 0.339s (36.8%)
    - Loss computation: 0.481s (52.2%)
    - Aggregation: 0.102s (11.0%)
  TOTALS:
    - Total computation: 0.921s (100.0%)
    - Total communication: 0.000s (0.0%)
    - Total parallel time: 0.921s

=== PER-NODE AVERAGE TIME ===
  - Distance computation: 0.302s
  - Loss computation: 0.349s
  - Aggregation: 0.057s
  - Model transfer: 0.000s
  - Total per node: 0.708s

=== TOTAL COMPUTATIONAL WORK (sum across all nodes) ===
  - Total distance computation: 30.205s
  - Total loss computation: 34.910s
  - Total aggregation: 5.708s
  - Total model transfer: 0.000s
  - Grand total: 70.823s
  - Mean Stage 1 acceptance rate: 0.500
  - Mean Stage 2 acceptance rate: 0.427
  - Overall acceptance rate: 0.214

UBAR Algorithm Properties:
  - Model dimension: 6,603,710
  - Rho parameter: 0.5
  - Two-stage approach: Distance filtering + loss evaluation
  - Stage 1 selects: 50% of neighbors
  - Stage 2 uses: Training sample loss comparison
  - Theoretical complexity: O(deg(i)×d + deg(i)×inference)
  - Approach: UBAR paper implementation


# Experiment completed successfully
