# Experiment: ubar k=154 dataset=femnist
# Timestamp: 2025-09-20T17:10:23.781249
# Device: CPU (forced for consistent performance)
# Command: python decentralized_fl_sim.py --dataset femnist --rounds 3 --local-epochs 1 --seed 987654321 --batch-size 64 --lr 0.01 --max-samples 10000 --agg ubar --ubar-rho 0.5 --attack-percentage 0.5 --attack-type directed_deviation --verbose --graph k-regular --k 154 --num-nodes 155
================================================================================

Device: cpu
Seed: 987654321
Loading 36 LEAF FEMNIST train files...
LEAF FEMNIST train: 3597 users, 734463 samples
Loading 36 LEAF FEMNIST test files...
LEAF FEMNIST test: 3597 users, 83388 samples
Found 3597 train users, 3597 test users, 3597 common users
User sample counts range: 525 (max) to 17 (min)
Distributed ALL 3597 users across 155 clients
Users per client: 23 (with 32 clients getting +1 user)
Train partition sizes: [5012, 4663, 5230, 4772, 5166, 4912, 4471, 4962, 5375, 5077, 5317, 4706, 5072, 5445, 5194, 4552, 4773, 5132, 5136, 4426, 4501, 5364, 5025, 4734, 5451, 4358, 5225, 5392, 4655, 4577, 5292, 4518, 5111, 4022, 4758, 4423, 5146, 3902, 5438, 4300, 5416, 4549, 4629, 4389, 4546, 5064, 4985, 4450, 5167, 4796, 3974, 5320, 4500, 4377, 4982, 4999, 4688, 4140, 3848, 4564, 4739, 5082, 4692, 4712, 4967, 4707, 5047, 4959, 5167, 4001, 4977, 5225, 4380, 4747, 4921, 4674, 4667, 4633, 5108, 4473, 4250, 4372, 4438, 4513, 4654, 4954, 4499, 4818, 4291, 4375, 4187, 4117, 4722, 4337, 4974, 4206, 4151, 4474, 4361, 4876, 4772, 5349, 4877, 5310, 4401, 5336, 4272, 4778, 5023, 4263, 5006, 4887, 4482, 5150, 5141, 4458, 4737, 4525, 5307, 4935, 4454, 4421, 4594, 4382, 4738, 4580, 4897, 5362, 4882, 4462, 5017, 4773, 4969, 4887, 5124, 4999, 4530, 4854, 5004, 4349, 4256, 4454, 4719, 3871, 4766, 4182, 4637, 4578, 4823, 4311, 4478, 4966, 4694, 4861, 4165]
Test partition sizes: [571, 529, 592, 541, 585, 558, 512, 562, 608, 577, 601, 533, 578, 616, 589, 518, 543, 583, 583, 505, 515, 610, 572, 535, 618, 495, 593, 611, 532, 522, 603, 512, 580, 460, 540, 504, 584, 446, 616, 490, 612, 517, 527, 498, 516, 573, 564, 507, 586, 543, 452, 605, 511, 497, 565, 566, 535, 469, 441, 517, 540, 578, 536, 535, 564, 536, 574, 564, 584, 455, 562, 592, 498, 541, 559, 531, 527, 527, 581, 508, 483, 495, 503, 512, 530, 562, 509, 548, 488, 495, 475, 466, 537, 493, 563, 477, 472, 508, 496, 552, 543, 604, 553, 601, 502, 603, 486, 542, 569, 488, 567, 557, 509, 580, 583, 506, 539, 511, 603, 560, 507, 501, 519, 498, 538, 520, 554, 607, 554, 505, 570, 543, 564, 553, 581, 568, 516, 553, 565, 495, 485, 502, 537, 442, 540, 474, 525, 520, 545, 490, 510, 565, 532, 552, 473]
  Client 0: 5012 train samples, 62 unique classes
  Client 1: 4663 train samples, 62 unique classes
  Client 2: 5230 train samples, 62 unique classes
  Client 3: 4772 train samples, 62 unique classes
  Client 4: 5166 train samples, 62 unique classes
  Client 5: 4912 train samples, 62 unique classes
  Client 6: 4471 train samples, 62 unique classes
  Client 7: 4962 train samples, 62 unique classes
  Client 8: 5375 train samples, 62 unique classes
  Client 9: 5077 train samples, 62 unique classes
  Client 10: 5317 train samples, 62 unique classes
  Client 11: 4706 train samples, 62 unique classes
  Client 12: 5072 train samples, 62 unique classes
  Client 13: 5445 train samples, 62 unique classes
  Client 14: 5194 train samples, 62 unique classes
  Client 15: 4552 train samples, 62 unique classes
  Client 16: 4773 train samples, 62 unique classes
  Client 17: 5132 train samples, 62 unique classes
  Client 18: 5136 train samples, 62 unique classes
  Client 19: 4426 train samples, 62 unique classes
  Client 20: 4501 train samples, 62 unique classes
  Client 21: 5364 train samples, 62 unique classes
  Client 22: 5025 train samples, 62 unique classes
  Client 23: 4734 train samples, 62 unique classes
  Client 24: 5451 train samples, 62 unique classes
  Client 25: 4358 train samples, 62 unique classes
  Client 26: 5225 train samples, 62 unique classes
  Client 27: 5392 train samples, 62 unique classes
  Client 28: 4655 train samples, 62 unique classes
  Client 29: 4577 train samples, 62 unique classes
  Client 30: 5292 train samples, 62 unique classes
  Client 31: 4518 train samples, 62 unique classes
  Client 32: 5111 train samples, 62 unique classes
  Client 33: 4022 train samples, 62 unique classes
  Client 34: 4758 train samples, 62 unique classes
  Client 35: 4423 train samples, 62 unique classes
  Client 36: 5146 train samples, 62 unique classes
  Client 37: 3902 train samples, 62 unique classes
  Client 38: 5438 train samples, 62 unique classes
  Client 39: 4300 train samples, 62 unique classes
  Client 40: 5416 train samples, 62 unique classes
  Client 41: 4549 train samples, 62 unique classes
  Client 42: 4629 train samples, 62 unique classes
  Client 43: 4389 train samples, 62 unique classes
  Client 44: 4546 train samples, 62 unique classes
  Client 45: 5064 train samples, 62 unique classes
  Client 46: 4985 train samples, 62 unique classes
  Client 47: 4450 train samples, 62 unique classes
  Client 48: 5167 train samples, 62 unique classes
  Client 49: 4796 train samples, 62 unique classes
  Client 50: 3974 train samples, 62 unique classes
  Client 51: 5320 train samples, 62 unique classes
  Client 52: 4500 train samples, 62 unique classes
  Client 53: 4377 train samples, 62 unique classes
  Client 54: 4982 train samples, 62 unique classes
  Client 55: 4999 train samples, 62 unique classes
  Client 56: 4688 train samples, 62 unique classes
  Client 57: 4140 train samples, 62 unique classes
  Client 58: 3848 train samples, 62 unique classes
  Client 59: 4564 train samples, 62 unique classes
  Client 60: 4739 train samples, 62 unique classes
  Client 61: 5082 train samples, 62 unique classes
  Client 62: 4692 train samples, 62 unique classes
  Client 63: 4712 train samples, 62 unique classes
  Client 64: 4967 train samples, 62 unique classes
  Client 65: 4707 train samples, 62 unique classes
  Client 66: 5047 train samples, 62 unique classes
  Client 67: 4959 train samples, 62 unique classes
  Client 68: 5167 train samples, 62 unique classes
  Client 69: 4001 train samples, 62 unique classes
  Client 70: 4977 train samples, 62 unique classes
  Client 71: 5225 train samples, 62 unique classes
  Client 72: 4380 train samples, 62 unique classes
  Client 73: 4747 train samples, 62 unique classes
  Client 74: 4921 train samples, 62 unique classes
  Client 75: 4674 train samples, 62 unique classes
  Client 76: 4667 train samples, 62 unique classes
  Client 77: 4633 train samples, 62 unique classes
  Client 78: 5108 train samples, 62 unique classes
  Client 79: 4473 train samples, 62 unique classes
  Client 80: 4250 train samples, 62 unique classes
  Client 81: 4372 train samples, 62 unique classes
  Client 82: 4438 train samples, 62 unique classes
  Client 83: 4513 train samples, 62 unique classes
  Client 84: 4654 train samples, 62 unique classes
  Client 85: 4954 train samples, 62 unique classes
  Client 86: 4499 train samples, 62 unique classes
  Client 87: 4818 train samples, 62 unique classes
  Client 88: 4291 train samples, 62 unique classes
  Client 89: 4375 train samples, 62 unique classes
  Client 90: 4187 train samples, 62 unique classes
  Client 91: 4117 train samples, 62 unique classes
  Client 92: 4722 train samples, 62 unique classes
  Client 93: 4337 train samples, 62 unique classes
  Client 94: 4974 train samples, 62 unique classes
  Client 95: 4206 train samples, 62 unique classes
  Client 96: 4151 train samples, 62 unique classes
  Client 97: 4474 train samples, 62 unique classes
  Client 98: 4361 train samples, 62 unique classes
  Client 99: 4876 train samples, 62 unique classes
  Client 100: 4772 train samples, 62 unique classes
  Client 101: 5349 train samples, 62 unique classes
  Client 102: 4877 train samples, 62 unique classes
  Client 103: 5310 train samples, 62 unique classes
  Client 104: 4401 train samples, 62 unique classes
  Client 105: 5336 train samples, 62 unique classes
  Client 106: 4272 train samples, 62 unique classes
  Client 107: 4778 train samples, 62 unique classes
  Client 108: 5023 train samples, 62 unique classes
  Client 109: 4263 train samples, 62 unique classes
  Client 110: 5006 train samples, 62 unique classes
  Client 111: 4887 train samples, 62 unique classes
  Client 112: 4482 train samples, 62 unique classes
  Client 113: 5150 train samples, 62 unique classes
  Client 114: 5141 train samples, 62 unique classes
  Client 115: 4458 train samples, 62 unique classes
  Client 116: 4737 train samples, 62 unique classes
  Client 117: 4525 train samples, 62 unique classes
  Client 118: 5307 train samples, 62 unique classes
  Client 119: 4935 train samples, 62 unique classes
  Client 120: 4454 train samples, 62 unique classes
  Client 121: 4421 train samples, 62 unique classes
  Client 122: 4594 train samples, 62 unique classes
  Client 123: 4382 train samples, 62 unique classes
  Client 124: 4738 train samples, 62 unique classes
  Client 125: 4580 train samples, 62 unique classes
  Client 126: 4897 train samples, 62 unique classes
  Client 127: 5362 train samples, 62 unique classes
  Client 128: 4882 train samples, 62 unique classes
  Client 129: 4462 train samples, 62 unique classes
  Client 130: 5017 train samples, 62 unique classes
  Client 131: 4773 train samples, 62 unique classes
  Client 132: 4969 train samples, 62 unique classes
  Client 133: 4887 train samples, 62 unique classes
  Client 134: 5124 train samples, 62 unique classes
  Client 135: 4999 train samples, 62 unique classes
  Client 136: 4530 train samples, 62 unique classes
  Client 137: 4854 train samples, 62 unique classes
  Client 138: 5004 train samples, 62 unique classes
  Client 139: 4349 train samples, 62 unique classes
  Client 140: 4256 train samples, 62 unique classes
  Client 141: 4454 train samples, 62 unique classes
  Client 142: 4719 train samples, 62 unique classes
  Client 143: 3871 train samples, 62 unique classes
  Client 144: 4766 train samples, 62 unique classes
  Client 145: 4182 train samples, 62 unique classes
  Client 146: 4637 train samples, 62 unique classes
  Client 147: 4578 train samples, 62 unique classes
  Client 148: 4823 train samples, 62 unique classes
  Client 149: 4311 train samples, 62 unique classes
  Client 150: 4478 train samples, 62 unique classes
  Client 151: 4966 train samples, 62 unique classes
  Client 152: 4694 train samples, 62 unique classes
  Client 153: 4861 train samples, 62 unique classes
  Client 154: 4165 train samples, 62 unique classes
Will sample 10000 samples per client per epoch
Graph: k-regular, nodes: 155, edges: 11935
Degree statistics: avg=154.00, min=154, max=154
k-regular with k=154 (each node has exactly 154 neighbors)
Attack: Compromised 77/155 nodes: [0, 2, 4, 9, 10, 13, 17, 27, 29, 30, 32, 35, 37, 38, 40, 43, 44, 46, 47, 48, 49, 50, 54, 55, 59, 61, 62, 64, 66, 67, 68, 73, 74, 76, 77, 78, 81, 83, 84, 91, 92, 94, 97, 98, 100, 101, 103, 104, 106, 107, 112, 113, 114, 115, 116, 118, 119, 120, 122, 124, 125, 127, 128, 130, 131, 132, 133, 134, 138, 141, 143, 144, 147, 149, 150, 152, 154]
Attack type: directed_deviation, lambda: 1.0
UBAR ALGORITHM (Two-Stage Byzantine-resilient)
  - Model dimension: 6,603,710 parameters
  - Rho parameter: 0.5
  - Stage 1: Distance-based filtering (select 50% closest neighbors)
  - Stage 2: Performance-based selection (loss comparison)
  - Complexity: O(deg(i)×d + deg(i)×inference)
Initial test acc across nodes: mean=0.0177 ± 0.0172
Round 001: test acc mean=0.0528 ± 0.0132 | min=0.0302 max=0.1041
         : test loss mean=4.0344 ± 0.0271
         : individual accs = ['0.049037', '0.060491', '0.045608', '0.057301', '0.034188', '0.062724', '0.048828', '0.056940', '0.065789', '0.046794', '0.049917', '0.065666', '0.051903', '0.035714', '0.064516', '0.040541', '0.042357', '0.046312', '0.034305', '0.055446', '0.050485', '0.042623', '0.052448', '0.044860', '0.042071', '0.046465', '0.057336', '0.037643', '0.060150', '0.051724', '0.036484', '0.060547', '0.037931', '0.071739', '0.055556', '0.077381', '0.097603', '0.065022', '0.037338', '0.104082', '0.040850', '0.048356', '0.049336', '0.050201', '0.048450', '0.043630', '0.042553', '0.057199', '0.040956', '0.040516', '0.055310', '0.077686', '0.099804', '0.046278', '0.053097', '0.049470', '0.039252', '0.042644', '0.061224', '0.046422', '0.037037', '0.057093', '0.035448', '0.063551', '0.046099', '0.050373', '0.043554', '0.047872', '0.054795', '0.057143', '0.046263', '0.042230', '0.058233', '0.048059', '0.042934', '0.050847', '0.037951', '0.041746', '0.056799', '0.059055', '0.074534', '0.054545', '0.061630', '0.093750', '0.045283', '0.042705', '0.056974', '0.043796', '0.061475', '0.066667', '0.054737', '0.075107', '0.067039', '0.070994', '0.030195', '0.056604', '0.044492', '0.064961', '0.052419', '0.059783', '0.049724', '0.034768', '0.050633', '0.034942', '0.049801', '0.036484', '0.047325', '0.047970', '0.047452', '0.057377', '0.072310', '0.075404', '0.058939', '0.063793', '0.036021', '0.055336', '0.048237', '0.056751', '0.041459', '0.044643', '0.043393', '0.053892', '0.055877', '0.056225', '0.042751', '0.048077', '0.048736', '0.031301', '0.074007', '0.059406', '0.049123', '0.036832', '0.060284', '0.059675', '0.058520', '0.033451', '0.048450', '0.066908', '0.040708', '0.050505', '0.047423', '0.039841', '0.067039', '0.061086', '0.046296', '0.061181', '0.053333', '0.048077', '0.062385', '0.089796', '0.054902', '0.033628', '0.046992', '0.043478', '0.063425']
         : correct/total = [(28, 571), (32, 529), (27, 592), (31, 541), (20, 585), (35, 558), (25, 512), (32, 562), (40, 608), (27, 577), (30, 601), (35, 533), (30, 578), (22, 616), (38, 589), (21, 518), (23, 543), (27, 583), (20, 583), (28, 505), (26, 515), (26, 610), (30, 572), (24, 535), (26, 618), (23, 495), (34, 593), (23, 611), (32, 532), (27, 522), (22, 603), (31, 512), (22, 580), (33, 460), (30, 540), (39, 504), (57, 584), (29, 446), (23, 616), (51, 490), (25, 612), (25, 517), (26, 527), (25, 498), (25, 516), (25, 573), (24, 564), (29, 507), (24, 586), (22, 543), (25, 452), (47, 605), (51, 511), (23, 497), (30, 565), (28, 566), (21, 535), (20, 469), (27, 441), (24, 517), (20, 540), (33, 578), (19, 536), (34, 535), (26, 564), (27, 536), (25, 574), (27, 564), (32, 584), (26, 455), (26, 562), (25, 592), (29, 498), (26, 541), (24, 559), (27, 531), (20, 527), (22, 527), (33, 581), (30, 508), (36, 483), (27, 495), (31, 503), (48, 512), (24, 530), (24, 562), (29, 509), (24, 548), (30, 488), (33, 495), (26, 475), (35, 466), (36, 537), (35, 493), (17, 563), (27, 477), (21, 472), (33, 508), (26, 496), (33, 552), (27, 543), (21, 604), (28, 553), (21, 601), (25, 502), (22, 603), (23, 486), (26, 542), (27, 569), (28, 488), (41, 567), (42, 557), (30, 509), (37, 580), (21, 583), (28, 506), (26, 539), (29, 511), (25, 603), (25, 560), (22, 507), (27, 501), (29, 519), (28, 498), (23, 538), (25, 520), (27, 554), (19, 607), (41, 554), (30, 505), (28, 570), (20, 543), (34, 564), (33, 553), (34, 581), (19, 568), (25, 516), (37, 553), (23, 565), (25, 495), (23, 485), (20, 502), (36, 537), (27, 442), (25, 540), (29, 474), (28, 525), (25, 520), (34, 545), (44, 490), (28, 510), (19, 565), (25, 532), (24, 552), (30, 473)]
         : compromised: 0.0498, honest: 0.0557
         : ubar stats = ['Node 0: s1=0.500, s2=0.130', 'Node 1: s1=0.500, s2=0.844', 'Node 2: s1=0.500, s2=0.403']...
Round 002: test acc mean=0.0518 ± 0.0117 | min=0.0270 max=0.1275
         : test loss mean=3.9867 ± 0.0234
         : individual accs = ['0.040280', '0.056711', '0.027027', '0.049908', '0.034188', '0.037634', '0.058594', '0.037367', '0.050987', '0.039861', '0.051581', '0.052533', '0.048443', '0.040584', '0.042445', '0.052124', '0.055249', '0.053173', '0.063465', '0.055446', '0.038835', '0.044262', '0.052448', '0.044860', '0.033981', '0.044444', '0.053963', '0.034370', '0.060150', '0.051724', '0.041459', '0.058594', '0.043103', '0.054348', '0.042593', '0.055556', '0.049658', '0.078475', '0.034091', '0.040816', '0.040850', '0.048356', '0.049336', '0.050201', '0.048450', '0.045375', '0.046099', '0.057199', '0.042662', '0.040516', '0.075221', '0.046281', '0.060665', '0.046278', '0.053097', '0.065371', '0.039252', '0.076759', '0.079365', '0.067698', '0.044444', '0.050173', '0.065299', '0.039252', '0.056738', '0.054104', '0.038328', '0.031915', '0.054795', '0.057143', '0.039146', '0.038851', '0.064257', '0.046211', '0.042934', '0.035782', '0.047438', '0.062619', '0.049914', '0.057087', '0.055901', '0.056566', '0.061630', '0.072266', '0.054717', '0.053381', '0.056974', '0.056569', '0.057377', '0.052525', '0.063158', '0.049356', '0.057728', '0.070994', '0.046181', '0.060797', '0.046610', '0.064961', '0.042339', '0.059783', '0.049724', '0.056291', '0.050633', '0.048253', '0.127490', '0.043118', '0.047325', '0.046125', '0.050967', '0.057377', '0.044092', '0.075404', '0.058939', '0.050000', '0.065180', '0.055336', '0.046382', '0.039139', '0.061360', '0.042857', '0.063116', '0.049900', '0.053950', '0.040161', '0.042751', '0.057692', '0.048736', '0.042834', '0.037906', '0.059406', '0.049123', '0.036832', '0.044326', '0.048825', '0.058520', '0.042254', '0.063953', '0.052441', '0.047788', '0.052525', '0.068041', '0.059761', '0.042831', '0.072398', '0.046296', '0.042194', '0.053333', '0.046154', '0.053211', '0.046939', '0.058824', '0.056637', '0.052632', '0.045290', '0.067653']
         : correct/total = [(23, 571), (30, 529), (16, 592), (27, 541), (20, 585), (21, 558), (30, 512), (21, 562), (31, 608), (23, 577), (31, 601), (28, 533), (28, 578), (25, 616), (25, 589), (27, 518), (30, 543), (31, 583), (37, 583), (28, 505), (20, 515), (27, 610), (30, 572), (24, 535), (21, 618), (22, 495), (32, 593), (21, 611), (32, 532), (27, 522), (25, 603), (30, 512), (25, 580), (25, 460), (23, 540), (28, 504), (29, 584), (35, 446), (21, 616), (20, 490), (25, 612), (25, 517), (26, 527), (25, 498), (25, 516), (26, 573), (26, 564), (29, 507), (25, 586), (22, 543), (34, 452), (28, 605), (31, 511), (23, 497), (30, 565), (37, 566), (21, 535), (36, 469), (35, 441), (35, 517), (24, 540), (29, 578), (35, 536), (21, 535), (32, 564), (29, 536), (22, 574), (18, 564), (32, 584), (26, 455), (22, 562), (23, 592), (32, 498), (25, 541), (24, 559), (19, 531), (25, 527), (33, 527), (29, 581), (29, 508), (27, 483), (28, 495), (31, 503), (37, 512), (29, 530), (30, 562), (29, 509), (31, 548), (28, 488), (26, 495), (30, 475), (23, 466), (31, 537), (35, 493), (26, 563), (29, 477), (22, 472), (33, 508), (21, 496), (33, 552), (27, 543), (34, 604), (28, 553), (29, 601), (64, 502), (26, 603), (23, 486), (25, 542), (29, 569), (28, 488), (25, 567), (42, 557), (30, 509), (29, 580), (38, 583), (28, 506), (25, 539), (20, 511), (37, 603), (24, 560), (32, 507), (25, 501), (28, 519), (20, 498), (23, 538), (30, 520), (27, 554), (26, 607), (21, 554), (30, 505), (28, 570), (20, 543), (25, 564), (27, 553), (34, 581), (24, 568), (33, 516), (29, 553), (27, 565), (26, 495), (33, 485), (30, 502), (23, 537), (32, 442), (25, 540), (20, 474), (28, 525), (24, 520), (29, 545), (23, 490), (30, 510), (32, 565), (28, 532), (25, 552), (32, 473)]
         : compromised: 0.0519, honest: 0.0517
         : ubar stats = ['Node 0: s1=0.500, s2=0.312', 'Node 1: s1=0.500, s2=0.487', 'Node 2: s1=0.500, s2=0.344']...
Round 003: test acc mean=0.0513 ± 0.0099 | min=0.0222 max=0.0771
         : test loss mean=3.9063 ± 0.0320
         : individual accs = ['0.043783', '0.062382', '0.038851', '0.060998', '0.051282', '0.062724', '0.058594', '0.040925', '0.052632', '0.051993', '0.051581', '0.037523', '0.048443', '0.040584', '0.042445', '0.069498', '0.064457', '0.053173', '0.068611', '0.055446', '0.044660', '0.044262', '0.071678', '0.033645', '0.042071', '0.046465', '0.057336', '0.058920', '0.060150', '0.051724', '0.043118', '0.068359', '0.043103', '0.050000', '0.050000', '0.051587', '0.058219', '0.051570', '0.060065', '0.061224', '0.040850', '0.048356', '0.049336', '0.060241', '0.048450', '0.045375', '0.053191', '0.057199', '0.049488', '0.051565', '0.064159', '0.046281', '0.052838', '0.062374', '0.047788', '0.065371', '0.039252', '0.076759', '0.077098', '0.050290', '0.050000', '0.050173', '0.050373', '0.039252', '0.042553', '0.054104', '0.038328', '0.031915', '0.044521', '0.065934', '0.046263', '0.042230', '0.058233', '0.051756', '0.042934', '0.067797', '0.049336', '0.053131', '0.030981', '0.045276', '0.074534', '0.054545', '0.061630', '0.072266', '0.039623', '0.053381', '0.056974', '0.047445', '0.051230', '0.060606', '0.054737', '0.045064', '0.048417', '0.046653', '0.047957', '0.067086', '0.044492', '0.039370', '0.054435', '0.059783', '0.049724', '0.056291', '0.039783', '0.034942', '0.053785', '0.038143', '0.047325', '0.042435', '0.050967', '0.057377', '0.044092', '0.050269', '0.058939', '0.044828', '0.065180', '0.047431', '0.055659', '0.052838', '0.061360', '0.033929', '0.063116', '0.037924', '0.059730', '0.068273', '0.042751', '0.044231', '0.037906', '0.046129', '0.059567', '0.053465', '0.049123', '0.038674', '0.053191', '0.045208', '0.041308', '0.042254', '0.063953', '0.066908', '0.053097', '0.050505', '0.053608', '0.041833', '0.044693', '0.052036', '0.022222', '0.061181', '0.053333', '0.038462', '0.034862', '0.046939', '0.047059', '0.056637', '0.054511', '0.057971', '0.063425']
         : correct/total = [(25, 571), (33, 529), (23, 592), (33, 541), (30, 585), (35, 558), (30, 512), (23, 562), (32, 608), (30, 577), (31, 601), (20, 533), (28, 578), (25, 616), (25, 589), (36, 518), (35, 543), (31, 583), (40, 583), (28, 505), (23, 515), (27, 610), (41, 572), (18, 535), (26, 618), (23, 495), (34, 593), (36, 611), (32, 532), (27, 522), (26, 603), (35, 512), (25, 580), (23, 460), (27, 540), (26, 504), (34, 584), (23, 446), (37, 616), (30, 490), (25, 612), (25, 517), (26, 527), (30, 498), (25, 516), (26, 573), (30, 564), (29, 507), (29, 586), (28, 543), (29, 452), (28, 605), (27, 511), (31, 497), (27, 565), (37, 566), (21, 535), (36, 469), (34, 441), (26, 517), (27, 540), (29, 578), (27, 536), (21, 535), (24, 564), (29, 536), (22, 574), (18, 564), (26, 584), (30, 455), (26, 562), (25, 592), (29, 498), (28, 541), (24, 559), (36, 531), (26, 527), (28, 527), (18, 581), (23, 508), (36, 483), (27, 495), (31, 503), (37, 512), (21, 530), (30, 562), (29, 509), (26, 548), (25, 488), (30, 495), (26, 475), (21, 466), (26, 537), (23, 493), (27, 563), (32, 477), (21, 472), (20, 508), (27, 496), (33, 552), (27, 543), (34, 604), (22, 553), (21, 601), (27, 502), (23, 603), (23, 486), (23, 542), (29, 569), (28, 488), (25, 567), (28, 557), (30, 509), (26, 580), (38, 583), (24, 506), (30, 539), (27, 511), (37, 603), (19, 560), (32, 507), (19, 501), (31, 519), (34, 498), (23, 538), (23, 520), (21, 554), (28, 607), (33, 554), (27, 505), (28, 570), (21, 543), (30, 564), (25, 553), (24, 581), (24, 568), (33, 516), (37, 553), (30, 565), (25, 495), (26, 485), (21, 502), (24, 537), (23, 442), (12, 540), (29, 474), (28, 525), (20, 520), (19, 545), (23, 490), (24, 510), (32, 565), (29, 532), (32, 552), (30, 473)]
         : compromised: 0.0491, honest: 0.0535
         : ubar stats = ['Node 0: s1=0.500, s2=0.442', 'Node 1: s1=0.500, s2=0.390', 'Node 2: s1=0.500, s2=0.494']...

=== FINAL RESULTS ===
Dataset: femnist, Nodes: 155, Graph: k-regular, Aggregation: ubar
Attack: directed_deviation, 50.0% compromised
Final accuracy - Compromised: 0.0491, Honest: 0.0535
Overall test accuracy: mean=0.0513 ± 0.0099

=== UBAR SUMMARY ===
Node 0: stage1=0.500, stage2=0.442, overall=0.221
Node 1: stage1=0.500, stage2=0.390, overall=0.195
Node 2: stage1=0.500, stage2=0.494, overall=0.247
Node 3: stage1=0.500, stage2=0.615, overall=0.307
Node 4: stage1=0.500, stage2=0.182, overall=0.091
Node 5: stage1=0.500, stage2=0.459, overall=0.229
Node 6: stage1=0.500, stage2=0.273, overall=0.136
Node 7: stage1=0.500, stage2=0.450, overall=0.225
Node 8: stage1=0.500, stage2=0.190, overall=0.095
Node 9: stage1=0.500, stage2=0.632, overall=0.316
Node 10: stage1=0.500, stage2=0.216, overall=0.108
Node 11: stage1=0.500, stage2=0.398, overall=0.199
Node 12: stage1=0.500, stage2=0.481, overall=0.240
Node 13: stage1=0.500, stage2=0.074, overall=0.037
Node 14: stage1=0.500, stage2=0.251, overall=0.126
Node 15: stage1=0.500, stage2=0.290, overall=0.145
Node 16: stage1=0.500, stage2=0.424, overall=0.212
Node 17: stage1=0.500, stage2=0.299, overall=0.149
Node 18: stage1=0.500, stage2=0.121, overall=0.061
Node 19: stage1=0.500, stage2=0.355, overall=0.177
Node 20: stage1=0.500, stage2=0.528, overall=0.264
Node 21: stage1=0.500, stage2=0.203, overall=0.102
Node 22: stage1=0.500, stage2=0.251, overall=0.126
Node 23: stage1=0.500, stage2=0.242, overall=0.121
Node 24: stage1=0.500, stage2=0.113, overall=0.056
Node 25: stage1=0.500, stage2=0.229, overall=0.115
Node 26: stage1=0.500, stage2=0.437, overall=0.219
Node 27: stage1=0.500, stage2=0.675, overall=0.338
Node 28: stage1=0.500, stage2=0.740, overall=0.370
Node 29: stage1=0.500, stage2=0.416, overall=0.208
Node 30: stage1=0.500, stage2=0.745, overall=0.372
Node 31: stage1=0.500, stage2=0.260, overall=0.130
Node 32: stage1=0.500, stage2=0.208, overall=0.104
Node 33: stage1=0.500, stage2=0.554, overall=0.277
Node 34: stage1=0.500, stage2=0.165, overall=0.082
Node 35: stage1=0.500, stage2=0.645, overall=0.323
Node 36: stage1=0.500, stage2=0.333, overall=0.167
Node 37: stage1=0.500, stage2=0.156, overall=0.078
Node 38: stage1=0.500, stage2=0.307, overall=0.154
Node 39: stage1=0.500, stage2=0.355, overall=0.177
Node 40: stage1=0.500, stage2=0.065, overall=0.032
Node 41: stage1=0.500, stage2=0.519, overall=0.260
Node 42: stage1=0.500, stage2=0.407, overall=0.203
Node 43: stage1=0.500, stage2=0.394, overall=0.197
Node 44: stage1=0.500, stage2=0.364, overall=0.182
Node 45: stage1=0.500, stage2=0.177, overall=0.089
Node 46: stage1=0.500, stage2=0.333, overall=0.167
Node 47: stage1=0.500, stage2=0.255, overall=0.128
Node 48: stage1=0.500, stage2=0.476, overall=0.238
Node 49: stage1=0.500, stage2=0.481, overall=0.240
Node 50: stage1=0.500, stage2=0.532, overall=0.266
Node 51: stage1=0.500, stage2=0.364, overall=0.182
Node 52: stage1=0.500, stage2=0.303, overall=0.152
Node 53: stage1=0.500, stage2=0.424, overall=0.212
Node 54: stage1=0.500, stage2=0.688, overall=0.344
Node 55: stage1=0.500, stage2=0.035, overall=0.017
Node 56: stage1=0.500, stage2=0.537, overall=0.268
Node 57: stage1=0.500, stage2=0.043, overall=0.022
Node 58: stage1=0.500, stage2=0.303, overall=0.152
Node 59: stage1=0.500, stage2=0.035, overall=0.017
Node 60: stage1=0.500, stage2=0.459, overall=0.229
Node 61: stage1=0.500, stage2=0.286, overall=0.143
Node 62: stage1=0.500, stage2=0.390, overall=0.195
Node 63: stage1=0.500, stage2=0.420, overall=0.210
Node 64: stage1=0.500, stage2=0.571, overall=0.286
Node 65: stage1=0.500, stage2=0.082, overall=0.041
Node 66: stage1=0.500, stage2=0.351, overall=0.175
Node 67: stage1=0.500, stage2=0.584, overall=0.292
Node 68: stage1=0.500, stage2=0.329, overall=0.165
Node 69: stage1=0.500, stage2=0.199, overall=0.100
Node 70: stage1=0.500, stage2=0.472, overall=0.236
Node 71: stage1=0.500, stage2=0.740, overall=0.370
Node 72: stage1=0.500, stage2=0.143, overall=0.071
Node 73: stage1=0.500, stage2=0.424, overall=0.212
Node 74: stage1=0.500, stage2=0.212, overall=0.106
Node 75: stage1=0.500, stage2=0.485, overall=0.242
Node 76: stage1=0.500, stage2=0.091, overall=0.045
Node 77: stage1=0.500, stage2=0.152, overall=0.076
Node 78: stage1=0.500, stage2=0.723, overall=0.361
Node 79: stage1=0.500, stage2=0.528, overall=0.264
Node 80: stage1=0.500, stage2=0.268, overall=0.134
Node 81: stage1=0.500, stage2=0.165, overall=0.082
Node 82: stage1=0.500, stage2=0.554, overall=0.277
Node 83: stage1=0.500, stage2=0.048, overall=0.024
Node 84: stage1=0.500, stage2=0.320, overall=0.160
Node 85: stage1=0.500, stage2=0.459, overall=0.229
Node 86: stage1=0.500, stage2=0.203, overall=0.102
Node 87: stage1=0.500, stage2=0.346, overall=0.173
Node 88: stage1=0.500, stage2=0.597, overall=0.299
Node 89: stage1=0.500, stage2=0.463, overall=0.232
Node 90: stage1=0.500, stage2=0.532, overall=0.266
Node 91: stage1=0.500, stage2=0.247, overall=0.123
Node 92: stage1=0.500, stage2=0.519, overall=0.260
Node 93: stage1=0.500, stage2=0.342, overall=0.171
Node 94: stage1=0.500, stage2=0.273, overall=0.136
Node 95: stage1=0.500, stage2=0.208, overall=0.104
Node 96: stage1=0.500, stage2=0.385, overall=0.193
Node 97: stage1=0.500, stage2=0.693, overall=0.346
Node 98: stage1=0.500, stage2=0.675, overall=0.338
Node 99: stage1=0.500, stage2=0.532, overall=0.266
Node 100: stage1=0.500, stage2=0.312, overall=0.156
Node 101: stage1=0.500, stage2=0.528, overall=0.264
Node 102: stage1=0.500, stage2=0.338, overall=0.169
Node 103: stage1=0.500, stage2=0.208, overall=0.104
Node 104: stage1=0.500, stage2=0.485, overall=0.242
Node 105: stage1=0.500, stage2=0.372, overall=0.186
Node 106: stage1=0.500, stage2=0.221, overall=0.110
Node 107: stage1=0.500, stage2=0.299, overall=0.149
Node 108: stage1=0.500, stage2=0.312, overall=0.156
Node 109: stage1=0.500, stage2=0.130, overall=0.065
Node 110: stage1=0.500, stage2=0.190, overall=0.095
Node 111: stage1=0.500, stage2=0.628, overall=0.314
Node 112: stage1=0.500, stage2=0.481, overall=0.240
Node 113: stage1=0.500, stage2=0.199, overall=0.100
Node 114: stage1=0.500, stage2=0.113, overall=0.056
Node 115: stage1=0.500, stage2=0.022, overall=0.011
Node 116: stage1=0.500, stage2=0.424, overall=0.212
Node 117: stage1=0.500, stage2=0.528, overall=0.264
Node 118: stage1=0.500, stage2=0.147, overall=0.074
Node 119: stage1=0.500, stage2=0.359, overall=0.180
Node 120: stage1=0.500, stage2=0.424, overall=0.212
Node 121: stage1=0.500, stage2=0.745, overall=0.372
Node 122: stage1=0.500, stage2=0.368, overall=0.184
Node 123: stage1=0.500, stage2=0.199, overall=0.100
Node 124: stage1=0.500, stage2=0.632, overall=0.316
Node 125: stage1=0.500, stage2=0.416, overall=0.208
Node 126: stage1=0.500, stage2=0.476, overall=0.238
Node 127: stage1=0.500, stage2=0.082, overall=0.041
Node 128: stage1=0.500, stage2=0.229, overall=0.115
Node 129: stage1=0.500, stage2=0.511, overall=0.255
Node 130: stage1=0.500, stage2=0.091, overall=0.045
Node 131: stage1=0.500, stage2=0.450, overall=0.225
Node 132: stage1=0.500, stage2=0.316, overall=0.158
Node 133: stage1=0.500, stage2=0.429, overall=0.214
Node 134: stage1=0.500, stage2=0.494, overall=0.247
Node 135: stage1=0.500, stage2=0.268, overall=0.134
Node 136: stage1=0.500, stage2=0.727, overall=0.364
Node 137: stage1=0.500, stage2=0.494, overall=0.247
Node 138: stage1=0.500, stage2=0.264, overall=0.132
Node 139: stage1=0.500, stage2=0.407, overall=0.203
Node 140: stage1=0.500, stage2=0.381, overall=0.190
Node 141: stage1=0.500, stage2=0.316, overall=0.158
Node 142: stage1=0.500, stage2=0.563, overall=0.281
Node 143: stage1=0.500, stage2=0.143, overall=0.071
Node 144: stage1=0.500, stage2=0.506, overall=0.253
Node 145: stage1=0.500, stage2=0.333, overall=0.167
Node 146: stage1=0.500, stage2=0.351, overall=0.175
Node 147: stage1=0.500, stage2=0.026, overall=0.013
Node 148: stage1=0.500, stage2=0.567, overall=0.284
Node 149: stage1=0.500, stage2=0.541, overall=0.271
Node 150: stage1=0.500, stage2=0.329, overall=0.165
Node 151: stage1=0.500, stage2=0.554, overall=0.277
Node 152: stage1=0.500, stage2=0.355, overall=0.177
Node 153: stage1=0.500, stage2=0.177, overall=0.089
Node 154: stage1=0.500, stage2=0.255, overall=0.128

=== PARALLEL EXECUTION TIME (realistic for distributed system) ===
  COMMUNICATION (max across nodes):
    - Full model transfer: 0.000s (0.0%)
  COMPUTATION (max across nodes):
    - Distance computation: 0.633s (42.3%)
    - Loss computation: 0.701s (46.8%)
    - Aggregation: 0.164s (11.0%)
  TOTALS:
    - Total computation: 1.499s (100.0%)
    - Total communication: 0.000s (0.0%)
    - Total parallel time: 1.499s

=== PER-NODE AVERAGE TIME ===
  - Distance computation: 0.461s
  - Loss computation: 0.561s
  - Aggregation: 0.074s
  - Model transfer: 0.000s
  - Total per node: 1.097s

=== TOTAL COMPUTATIONAL WORK (sum across all nodes) ===
  - Total distance computation: 71.498s
  - Total loss computation: 87.014s
  - Total aggregation: 11.447s
  - Total model transfer: 0.000s
  - Grand total: 169.958s
  - Mean Stage 1 acceptance rate: 0.500
  - Mean Stage 2 acceptance rate: 0.365
  - Overall acceptance rate: 0.182

UBAR Algorithm Properties:
  - Model dimension: 6,603,710
  - Rho parameter: 0.5
  - Two-stage approach: Distance filtering + loss evaluation
  - Stage 1 selects: 50% of neighbors
  - Stage 2 uses: Training sample loss comparison
  - Theoretical complexity: O(deg(i)×d + deg(i)×inference)
  - Approach: UBAR paper implementation


# Experiment completed successfully
