# Experiment: balance k=96 dataset=femnist
# Timestamp: 2025-09-19T15:26:43.230266
# Command: python decentralized_fl_sim.py --dataset femnist --rounds 3 --local-epochs 1 --seed 987654321 --batch-size 64 --lr 0.01 --max-samples 10000 --agg balance --attack-percentage 0.5 --attack-type directed_deviation --verbose --graph k-regular --k 96 --num-nodes 100
================================================================================

CUDAGraph supports dynamic shapes by recording a new graph for each distinct input size. Recording too many CUDAGraphs may lead to extra overhead. We have observed 51 distinct sizes. Please consider the following options for better performance: a) padding inputs to a few fixed number of shapes; or b) set torch._inductor.config.triton.cudagraph_skip_dynamic_graphs=True. Set torch._inductor.config.triton.cudagraph_dynamic_shape_warn_limit=None to silence this warning.
Device: cuda
Seed: 987654321
Loading 36 LEAF FEMNIST train files...
LEAF FEMNIST train: 3597 users, 734463 samples
Loading 36 LEAF FEMNIST test files...
LEAF FEMNIST test: 3597 users, 83388 samples
Found 3597 train users, 3597 test users, 3597 common users
User sample counts range: 525 (max) to 17 (min)
Distributed ALL 3597 users across 100 clients
Users per client: 35 (with 97 clients getting +1 user)
Train partition sizes: [8056, 7245, 7634, 7059, 7480, 6831, 7408, 8154, 7361, 7470, 6246, 6823, 7701, 6799, 7175, 7379, 7191, 7366, 6829, 7248, 6921, 6718, 7193, 8029, 7759, 7612, 7427, 7763, 6890, 8166, 7757, 7698, 8091, 7672, 7300, 6646, 7186, 7943, 8164, 7471, 7696, 7796, 7032, 8059, 7171, 8263, 7313, 6952, 7895, 7620, 7505, 7410, 6969, 7307, 7020, 7166, 7229, 7423, 7740, 7298, 6666, 7173, 6919, 7659, 6658, 7010, 7176, 8216, 7331, 7127, 7472, 7191, 6756, 6957, 6837, 7074, 7606, 7168, 6604, 6586, 7021, 8138, 7179, 7456, 6956, 8016, 7597, 7243, 7390, 7300, 7527, 7793, 6490, 8266, 7349, 7188, 7373, 6901, 6627, 7742]
Test partition sizes: [913, 824, 868, 802, 850, 776, 844, 924, 837, 847, 711, 776, 872, 774, 814, 835, 818, 836, 778, 822, 787, 762, 818, 909, 881, 866, 845, 879, 785, 929, 880, 873, 918, 869, 827, 755, 811, 900, 921, 849, 874, 883, 796, 909, 813, 936, 828, 790, 896, 863, 849, 843, 791, 829, 799, 816, 823, 843, 875, 830, 754, 813, 787, 869, 757, 799, 814, 934, 831, 809, 849, 818, 771, 792, 777, 806, 865, 816, 752, 747, 800, 921, 812, 844, 790, 910, 862, 822, 842, 829, 854, 883, 741, 933, 835, 817, 836, 788, 758, 880]
  Client 0: 8056 train samples, 62 unique classes
  Client 1: 7245 train samples, 62 unique classes
  Client 2: 7634 train samples, 62 unique classes
  Client 3: 7059 train samples, 62 unique classes
  Client 4: 7480 train samples, 62 unique classes
  Client 5: 6831 train samples, 62 unique classes
  Client 6: 7408 train samples, 62 unique classes
  Client 7: 8154 train samples, 62 unique classes
  Client 8: 7361 train samples, 62 unique classes
  Client 9: 7470 train samples, 62 unique classes
  Client 10: 6246 train samples, 62 unique classes
  Client 11: 6823 train samples, 62 unique classes
  Client 12: 7701 train samples, 62 unique classes
  Client 13: 6799 train samples, 62 unique classes
  Client 14: 7175 train samples, 62 unique classes
  Client 15: 7379 train samples, 62 unique classes
  Client 16: 7191 train samples, 62 unique classes
  Client 17: 7366 train samples, 62 unique classes
  Client 18: 6829 train samples, 62 unique classes
  Client 19: 7248 train samples, 62 unique classes
  Client 20: 6921 train samples, 62 unique classes
  Client 21: 6718 train samples, 62 unique classes
  Client 22: 7193 train samples, 62 unique classes
  Client 23: 8029 train samples, 62 unique classes
  Client 24: 7759 train samples, 62 unique classes
  Client 25: 7612 train samples, 62 unique classes
  Client 26: 7427 train samples, 62 unique classes
  Client 27: 7763 train samples, 62 unique classes
  Client 28: 6890 train samples, 62 unique classes
  Client 29: 8166 train samples, 62 unique classes
  Client 30: 7757 train samples, 62 unique classes
  Client 31: 7698 train samples, 62 unique classes
  Client 32: 8091 train samples, 62 unique classes
  Client 33: 7672 train samples, 62 unique classes
  Client 34: 7300 train samples, 62 unique classes
  Client 35: 6646 train samples, 62 unique classes
  Client 36: 7186 train samples, 62 unique classes
  Client 37: 7943 train samples, 62 unique classes
  Client 38: 8164 train samples, 62 unique classes
  Client 39: 7471 train samples, 62 unique classes
  Client 40: 7696 train samples, 62 unique classes
  Client 41: 7796 train samples, 62 unique classes
  Client 42: 7032 train samples, 62 unique classes
  Client 43: 8059 train samples, 62 unique classes
  Client 44: 7171 train samples, 62 unique classes
  Client 45: 8263 train samples, 62 unique classes
  Client 46: 7313 train samples, 62 unique classes
  Client 47: 6952 train samples, 62 unique classes
  Client 48: 7895 train samples, 62 unique classes
  Client 49: 7620 train samples, 62 unique classes
  Client 50: 7505 train samples, 62 unique classes
  Client 51: 7410 train samples, 62 unique classes
  Client 52: 6969 train samples, 62 unique classes
  Client 53: 7307 train samples, 62 unique classes
  Client 54: 7020 train samples, 62 unique classes
  Client 55: 7166 train samples, 62 unique classes
  Client 56: 7229 train samples, 62 unique classes
  Client 57: 7423 train samples, 62 unique classes
  Client 58: 7740 train samples, 62 unique classes
  Client 59: 7298 train samples, 62 unique classes
  Client 60: 6666 train samples, 62 unique classes
  Client 61: 7173 train samples, 62 unique classes
  Client 62: 6919 train samples, 62 unique classes
  Client 63: 7659 train samples, 62 unique classes
  Client 64: 6658 train samples, 62 unique classes
  Client 65: 7010 train samples, 62 unique classes
  Client 66: 7176 train samples, 62 unique classes
  Client 67: 8216 train samples, 62 unique classes
  Client 68: 7331 train samples, 62 unique classes
  Client 69: 7127 train samples, 62 unique classes
  Client 70: 7472 train samples, 62 unique classes
  Client 71: 7191 train samples, 62 unique classes
  Client 72: 6756 train samples, 62 unique classes
  Client 73: 6957 train samples, 62 unique classes
  Client 74: 6837 train samples, 62 unique classes
  Client 75: 7074 train samples, 62 unique classes
  Client 76: 7606 train samples, 62 unique classes
  Client 77: 7168 train samples, 62 unique classes
  Client 78: 6604 train samples, 62 unique classes
  Client 79: 6586 train samples, 62 unique classes
  Client 80: 7021 train samples, 62 unique classes
  Client 81: 8138 train samples, 62 unique classes
  Client 82: 7179 train samples, 62 unique classes
  Client 83: 7456 train samples, 62 unique classes
  Client 84: 6956 train samples, 62 unique classes
  Client 85: 8016 train samples, 62 unique classes
  Client 86: 7597 train samples, 62 unique classes
  Client 87: 7243 train samples, 62 unique classes
  Client 88: 7390 train samples, 62 unique classes
  Client 89: 7300 train samples, 62 unique classes
  Client 90: 7527 train samples, 62 unique classes
  Client 91: 7793 train samples, 62 unique classes
  Client 92: 6490 train samples, 62 unique classes
  Client 93: 8266 train samples, 62 unique classes
  Client 94: 7349 train samples, 62 unique classes
  Client 95: 7188 train samples, 62 unique classes
  Client 96: 7373 train samples, 62 unique classes
  Client 97: 6901 train samples, 62 unique classes
  Client 98: 6627 train samples, 62 unique classes
  Client 99: 7742 train samples, 62 unique classes
Will sample 10000 samples per client per epoch
Graph: k-regular, nodes: 100, edges: 4800
Degree statistics: avg=96.00, min=96, max=96
k-regular with k=96 (each node has exactly 96 neighbors)
Attack: Compromised 50/100 nodes: [0, 4, 6, 10, 13, 14, 15, 16, 18, 19, 22, 23, 24, 25, 27, 29, 30, 31, 32, 33, 35, 38, 40, 41, 45, 46, 47, 48, 49, 50, 52, 53, 55, 56, 57, 59, 61, 62, 66, 68, 73, 76, 77, 84, 85, 87, 88, 90, 92, 99]
Attack type: directed_deviation, lambda: 1.0
BALANCE algorithm:
  - Model dimension: 6,603,710 parameters
  - Complexity: O(N×d) = O(100×6,603,710)
Initial test acc across nodes: mean=0.0186 ± 0.0179
CUDAGraph supports dynamic shapes by recording a new graph for each distinct input size. Recording too many CUDAGraphs may lead to extra overhead. We have observed 51 distinct sizes. Please consider the following options for better performance: a) padding inputs to a few fixed number of shapes; or b) set torch._inductor.config.triton.cudagraph_skip_dynamic_graphs=True. Set torch._inductor.config.triton.cudagraph_dynamic_shape_warn_limit=None to silence this warning.
CUDAGraph supports dynamic shapes by recording a new graph for each distinct input size. Recording too many CUDAGraphs may lead to extra overhead. We have observed 51 distinct sizes. Please consider the following options for better performance: a) padding inputs to a few fixed number of shapes; or b) set torch._inductor.config.triton.cudagraph_skip_dynamic_graphs=True. Set torch._inductor.config.triton.cudagraph_dynamic_shape_warn_limit=None to silence this warning.
Round 001: test acc mean=0.0583 ± 0.0177 | min=0.0342 max=0.1201
         : test loss mean=4.0401 ± 0.0102
         : individual accs = ['0.050383', '0.041262', '0.094470', '0.081047', '0.057647', '0.063144', '0.049763', '0.035714', '0.074074', '0.044864', '0.090014', '0.064433', '0.037844', '0.047804', '0.060197', '0.034731', '0.050122', '0.065789', '0.065553', '0.053528', '0.052097', '0.051181', '0.041565', '0.035204', '0.048808', '0.058891', '0.057988', '0.055745', '0.047134', '0.051668', '0.043182', '0.084765', '0.044662', '0.059839', '0.050786', '0.103311', '0.106042', '0.045556', '0.074919', '0.064782', '0.045767', '0.041903', '0.070352', '0.088009', '0.045510', '0.034188', '0.053140', '0.054430', '0.049107', '0.056779', '0.061249', '0.052195', '0.067004', '0.060314', '0.102628', '0.120098', '0.068044', '0.051008', '0.060571', '0.046988', '0.053050', '0.050431', '0.048285', '0.057537', '0.070013', '0.045056', '0.047912', '0.052463', '0.055355', '0.044499', '0.057715', '0.050122', '0.058366', '0.044192', '0.043758', '0.044665', '0.045087', '0.050245', '0.062500', '0.115127', '0.057500', '0.051031', '0.061576', '0.050948', '0.051899', '0.041758', '0.044084', '0.059611', '0.071259', '0.051870', '0.039813', '0.048698', '0.095816', '0.045016', '0.069461', '0.047736', '0.041866', '0.064721', '0.068602', '0.096591']
         : correct/total = [(46, 913), (34, 824), (82, 868), (65, 802), (49, 850), (49, 776), (42, 844), (33, 924), (62, 837), (38, 847), (64, 711), (50, 776), (33, 872), (37, 774), (49, 814), (29, 835), (41, 818), (55, 836), (51, 778), (44, 822), (41, 787), (39, 762), (34, 818), (32, 909), (43, 881), (51, 866), (49, 845), (49, 879), (37, 785), (48, 929), (38, 880), (74, 873), (41, 918), (52, 869), (42, 827), (78, 755), (86, 811), (41, 900), (69, 921), (55, 849), (40, 874), (37, 883), (56, 796), (80, 909), (37, 813), (32, 936), (44, 828), (43, 790), (44, 896), (49, 863), (52, 849), (44, 843), (53, 791), (50, 829), (82, 799), (98, 816), (56, 823), (43, 843), (53, 875), (39, 830), (40, 754), (41, 813), (38, 787), (50, 869), (53, 757), (36, 799), (39, 814), (49, 934), (46, 831), (36, 809), (49, 849), (41, 818), (45, 771), (35, 792), (34, 777), (36, 806), (39, 865), (41, 816), (47, 752), (86, 747), (46, 800), (47, 921), (50, 812), (43, 844), (41, 790), (38, 910), (38, 862), (49, 822), (60, 842), (43, 829), (34, 854), (43, 883), (71, 741), (42, 933), (58, 835), (39, 817), (35, 836), (51, 788), (52, 758), (85, 880)]
         : compromised: 0.0574, honest: 0.0591
Round 002: test acc mean=0.0520 ± 0.0102 | min=0.0323 max=0.0956
         : test loss mean=3.9238 ± 0.0280
         : individual accs = ['0.050383', '0.050971', '0.032258', '0.057357', '0.045882', '0.063144', '0.055687', '0.033550', '0.095579', '0.054309', '0.059072', '0.055412', '0.041284', '0.034884', '0.057740', '0.052695', '0.050122', '0.040670', '0.075835', '0.053528', '0.052097', '0.055118', '0.068460', '0.050605', '0.048808', '0.045035', '0.063905', '0.037543', '0.059873', '0.051668', '0.046591', '0.057274', '0.041394', '0.051784', '0.050786', '0.063576', '0.069051', '0.040000', '0.056460', '0.055359', '0.045767', '0.057758', '0.042714', '0.042904', '0.045510', '0.047009', '0.059179', '0.059494', '0.040179', '0.038239', '0.051826', '0.052195', '0.055626', '0.044632', '0.063830', '0.064951', '0.047388', '0.051008', '0.036571', '0.046988', '0.053050', '0.061501', '0.048285', '0.046030', '0.056803', '0.052566', '0.046683', '0.046039', '0.055355', '0.055624', '0.083628', '0.050122', '0.063554', '0.039141', '0.038610', '0.049628', '0.035838', '0.058824', '0.062500', '0.050870', '0.045000', '0.048860', '0.055419', '0.050948', '0.053165', '0.041758', '0.046404', '0.064477', '0.055819', '0.048251', '0.039813', '0.046433', '0.060729', '0.041801', '0.069461', '0.061200', '0.040670', '0.045685', '0.056728', '0.053409']
         : correct/total = [(46, 913), (42, 824), (28, 868), (46, 802), (39, 850), (49, 776), (47, 844), (31, 924), (80, 837), (46, 847), (42, 711), (43, 776), (36, 872), (27, 774), (47, 814), (44, 835), (41, 818), (34, 836), (59, 778), (44, 822), (41, 787), (42, 762), (56, 818), (46, 909), (43, 881), (39, 866), (54, 845), (33, 879), (47, 785), (48, 929), (41, 880), (50, 873), (38, 918), (45, 869), (42, 827), (48, 755), (56, 811), (36, 900), (52, 921), (47, 849), (40, 874), (51, 883), (34, 796), (39, 909), (37, 813), (44, 936), (49, 828), (47, 790), (36, 896), (33, 863), (44, 849), (44, 843), (44, 791), (37, 829), (51, 799), (53, 816), (39, 823), (43, 843), (32, 875), (39, 830), (40, 754), (50, 813), (38, 787), (40, 869), (43, 757), (42, 799), (38, 814), (43, 934), (46, 831), (45, 809), (71, 849), (41, 818), (49, 771), (31, 792), (30, 777), (40, 806), (31, 865), (48, 816), (47, 752), (38, 747), (36, 800), (45, 921), (45, 812), (43, 844), (42, 790), (38, 910), (40, 862), (53, 822), (47, 842), (40, 829), (34, 854), (41, 883), (45, 741), (39, 933), (58, 835), (50, 817), (34, 836), (36, 788), (43, 758), (47, 880)]
         : compromised: 0.0516, honest: 0.0524
Round 003: test acc mean=0.0506 ± 0.0087 | min=0.0309 max=0.0926
         : test loss mean=3.7513 ± 0.0398
         : individual accs = ['0.050383', '0.041262', '0.064516', '0.058603', '0.057647', '0.039948', '0.049763', '0.033550', '0.050179', '0.044864', '0.059072', '0.048969', '0.047018', '0.062016', '0.052826', '0.034731', '0.050122', '0.046651', '0.055270', '0.055961', '0.054638', '0.055118', '0.068460', '0.050605', '0.048808', '0.045035', '0.063905', '0.039818', '0.042038', '0.092573', '0.048864', '0.054983', '0.041394', '0.074799', '0.050786', '0.063576', '0.043157', '0.045556', '0.056460', '0.050648', '0.045767', '0.041903', '0.042714', '0.044004', '0.057811', '0.043803', '0.059179', '0.055696', '0.049107', '0.038239', '0.051826', '0.052195', '0.055626', '0.063932', '0.047559', '0.047794', '0.047388', '0.046263', '0.049143', '0.057831', '0.053050', '0.059041', '0.048285', '0.046030', '0.043593', '0.058824', '0.047912', '0.050321', '0.055355', '0.053152', '0.045936', '0.062347', '0.058366', '0.051768', '0.043758', '0.044665', '0.045087', '0.042892', '0.062500', '0.050870', '0.045000', '0.049946', '0.055419', '0.054502', '0.048101', '0.051648', '0.046404', '0.049878', '0.030879', '0.048251', '0.039813', '0.046433', '0.060729', '0.041801', '0.046707', '0.040392', '0.041866', '0.045685', '0.056728', '0.043182']
         : correct/total = [(46, 913), (34, 824), (56, 868), (47, 802), (49, 850), (31, 776), (42, 844), (31, 924), (42, 837), (38, 847), (42, 711), (38, 776), (41, 872), (48, 774), (43, 814), (29, 835), (41, 818), (39, 836), (43, 778), (46, 822), (43, 787), (42, 762), (56, 818), (46, 909), (43, 881), (39, 866), (54, 845), (35, 879), (33, 785), (86, 929), (43, 880), (48, 873), (38, 918), (65, 869), (42, 827), (48, 755), (35, 811), (41, 900), (52, 921), (43, 849), (40, 874), (37, 883), (34, 796), (40, 909), (47, 813), (41, 936), (49, 828), (44, 790), (44, 896), (33, 863), (44, 849), (44, 843), (44, 791), (53, 829), (38, 799), (39, 816), (39, 823), (39, 843), (43, 875), (48, 830), (40, 754), (48, 813), (38, 787), (40, 869), (33, 757), (47, 799), (39, 814), (47, 934), (46, 831), (43, 809), (39, 849), (51, 818), (45, 771), (41, 792), (34, 777), (36, 806), (39, 865), (35, 816), (47, 752), (38, 747), (36, 800), (46, 921), (45, 812), (46, 844), (38, 790), (47, 910), (40, 862), (41, 822), (26, 842), (40, 829), (34, 854), (41, 883), (45, 741), (39, 933), (39, 835), (33, 817), (35, 836), (36, 788), (43, 758), (38, 880)]
         : compromised: 0.0518, honest: 0.0493

=== FINAL RESULTS ===
Dataset: femnist, Nodes: 100, Graph: k-regular, Aggregation: balance
Attack: directed_deviation, 50.0% compromised
Final accuracy - Compromised: 0.0518, Honest: 0.0493
Overall test accuracy: mean=0.0506 ± 0.0087

=== BALANCE SUMMARY ===
Node 0: acceptance=0.170
Node 1: acceptance=0.167
Node 2: acceptance=0.167
Node 3: acceptance=0.167
Node 4: acceptance=0.170
Node 5: acceptance=0.167
Node 6: acceptance=0.174
Node 7: acceptance=0.170
Node 8: acceptance=0.170
Node 9: acceptance=0.163
Node 10: acceptance=0.170
Node 11: acceptance=0.167
Node 12: acceptance=0.167
Node 13: acceptance=0.167
Node 14: acceptance=0.163
Node 15: acceptance=0.167
Node 16: acceptance=0.167
Node 17: acceptance=0.167
Node 18: acceptance=0.167
Node 19: acceptance=0.167
Node 20: acceptance=0.160
Node 21: acceptance=0.160
Node 22: acceptance=0.167
Node 23: acceptance=0.167
Node 24: acceptance=0.167
Node 25: acceptance=0.167
Node 26: acceptance=0.167
Node 27: acceptance=0.170
Node 28: acceptance=0.163
Node 29: acceptance=0.163
Node 30: acceptance=0.163
Node 31: acceptance=0.163
Node 32: acceptance=0.163
Node 33: acceptance=0.167
Node 34: acceptance=0.167
Node 35: acceptance=0.170
Node 36: acceptance=0.167
Node 37: acceptance=0.167
Node 38: acceptance=0.170
Node 39: acceptance=0.167
Node 40: acceptance=0.167
Node 41: acceptance=0.170
Node 42: acceptance=0.163
Node 43: acceptance=0.163
Node 44: acceptance=0.160
Node 45: acceptance=0.163
Node 46: acceptance=0.163
Node 47: acceptance=0.163
Node 48: acceptance=0.167
Node 49: acceptance=0.170
Node 50: acceptance=0.170
Node 51: acceptance=0.163
Node 52: acceptance=0.163
Node 53: acceptance=0.167
Node 54: acceptance=0.163
Node 55: acceptance=0.170
Node 56: acceptance=0.167
Node 57: acceptance=0.167
Node 58: acceptance=0.160
Node 59: acceptance=0.167
Node 60: acceptance=0.163
Node 61: acceptance=0.167
Node 62: acceptance=0.167
Node 63: acceptance=0.167
Node 64: acceptance=0.170
Node 65: acceptance=0.170
Node 66: acceptance=0.170
Node 67: acceptance=0.170
Node 68: acceptance=0.170
Node 69: acceptance=0.167
Node 70: acceptance=0.167
Node 71: acceptance=0.163
Node 72: acceptance=0.167
Node 73: acceptance=0.174
Node 74: acceptance=0.170
Node 75: acceptance=0.167
Node 76: acceptance=0.170
Node 77: acceptance=0.167
Node 78: acceptance=0.167
Node 79: acceptance=0.167
Node 80: acceptance=0.170
Node 81: acceptance=0.174
Node 82: acceptance=0.170
Node 83: acceptance=0.167
Node 84: acceptance=0.170
Node 85: acceptance=0.167
Node 86: acceptance=0.163
Node 87: acceptance=0.167
Node 88: acceptance=0.167
Node 89: acceptance=0.167
Node 90: acceptance=0.170
Node 91: acceptance=0.170
Node 92: acceptance=0.167
Node 93: acceptance=0.160
Node 94: acceptance=0.163
Node 95: acceptance=0.167
Node 96: acceptance=0.170
Node 97: acceptance=0.170
Node 98: acceptance=0.170
Node 99: acceptance=0.174

=== PARALLEL EXECUTION TIME (realistic for distributed system) ===
  COMMUNICATION (max across nodes):
    - Full model transfer: 0.000s (0.0%)
  COMPUTATION (max across nodes):
    - Filtering: 0.119s (91.9%)
    - Aggregation: 0.010s (8.1%)
  TOTALS:
    - Total computation: 0.129s (100.0%)
    - Total communication: 0.000s (0.0%)
    - Total parallel time: 0.129s

=== PER-NODE AVERAGE TIME ===
  - Filtering: 0.111s
  - Aggregation: 0.003s
  - Model transfer: 0.000s
  - Total per node: 0.114s

=== TOTAL COMPUTATIONAL WORK (sum across all nodes) ===
  - Total filtering: 11.051s
  - Total aggregation: 0.343s
  - Total model transfer: 0.000s
  - Grand total: 11.394s
  - Mean acceptance rate: 0.167

BALANCE Algorithm Properties:
  - Model dimension: 6,603,710
  - No compression: Full parameter comparison
  - Theoretical complexity: O(deg(i)×d)
  - Approach: Full parameter filtering + averaging


# Experiment completed successfully
